{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from config import conv_map, device, operators\n",
    "\n",
    "from fontmodel import (FontModel, TransformerDecoder, DecodeInstruction,\n",
    "                        DecodeType, SamplingType, TransformerScheduler)\n",
    "from op_vae import OpVAE\n",
    "from dataset_loader import BucketedDataset\n",
    "from tokenizer import Tokenizer\n",
    "from glyph_viz import Visualizer\n",
    "from performance import PerformanceMetrics\n",
    "from tablelist_utils import numbers_first, make_non_cumulative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Executing train-cff.ipynb on {device}...\\n-----------------------------\")\n",
    "\n",
    "args = {\n",
    "    \"load_model\": False,\n",
    "    \"train_transformer\": True,\n",
    "    \"min_number\": -500,\n",
    "    \"max_number\": 500,\n",
    "    \"max_seq_len\": 5040,\n",
    "    \"num_layers\": 6,\n",
    "    \"embedding_dim\": 512,\n",
    "    \"num_heads\": 16,\n",
    "    \"ff_dim\": 768 * 2,\n",
    "    \"use_wandb\": True,\n",
    "    \"epochs\": 25,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 1e-4,\n",
    "    \"dropout_rate\": 0.0,\n",
    "    \"weight_decay\": 1e-1,\n",
    "    \"gradient_clip\": True,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"label_smoothing\": 0.1,\n",
    "    \"sample_every\": 1,\n",
    "    \"use_scheduler\": True,\n",
    "    \"scheduler_warmup_steps\": 5000,\n",
    "    \"data_type\": torch.bfloat16,\n",
    "    \"vae_beta\": 1e-1,\n",
    "    \"vae_epochs\": 10,\n",
    "    \"vae_lr\": 1e-2,\n",
    "    \"vae_weight_decay\": 1e-5,\n",
    "}\n",
    "\n",
    "print(\"Training hyperparameters:\")\n",
    "pprint(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = \"<PAD>\"\n",
    "sos_token = \"<SOS>\"\n",
    "eos_token = \"<EOS>\"\n",
    "tokenizer = Tokenizer(\n",
    "    min_number=args['min_number'],\n",
    "    max_number=args['max_number'],\n",
    "    possible_operators=operators,\n",
    "    pad_token=pad_token,\n",
    "    sos_token=sos_token,\n",
    "    eos_token=eos_token\n",
    ")\n",
    "cumulative = True\n",
    "vocab_size = tokenizer.num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_instr = DecodeInstruction( # NOTE: doesn't matter unless loading from .config.txt fails\n",
    "    DecodeType.ANCESTRAL,\n",
    "    SamplingType.GREEDY,\n",
    "    max_seq_len=args['max_seq_len'],\n",
    "    k=5,\n",
    "    p=0,\n",
    "    temp=0,\n",
    "    beam_size=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['load_model']:\n",
    "    model = torch.load(f'models/ldm-basic-35851allchars-0.pkl', map_location=device).to(device)\n",
    "else:\n",
    "    model = FontModel(\n",
    "        num_enc_layers=args['num_layers'],\n",
    "        num_dec_layers=args['num_layers'],\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=args['embedding_dim'],\n",
    "        num_heads=args['num_heads'],\n",
    "        ff_dim=args['ff_dim'],\n",
    "        dropout_rate=args['dropout_rate'],\n",
    "        max_seq_len=args['max_seq_len'],\n",
    "        device=device\n",
    "    ).to(device, dtype=args['data_type'])\n",
    "\n",
    "    # op_vae = OpVAE(\n",
    "    #     vocab_size=vocab_size,\n",
    "    #     embedding_dim=args['embedding_dim'],\n",
    "    #     num_layers=1,\n",
    "    #     hidden_dim=128,\n",
    "    #     bidirectional=True\n",
    "    # ).to(device, dtype=args['data_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(modela):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, modela.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print(f\"Number of trainable parameters: {params}\")\n",
    "    return params\n",
    "\n",
    "x = count_params(model)\n",
    "y = count_params(model.encoder)\n",
    "z = count_params(model.decoder)\n",
    "w = count_params(model.encoder.embedder)\n",
    "v = count_params(model.embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (tentative):\n",
    "# FontModel: embedder (DON'T APPLY WEIGHT DECAY)\n",
    "# TransformerDecoder: transformer_decoder_layers (DON'T APPLY WEIGHT DECAY TO RMSNORM), command_encoder, command_decoder, norm_final (DON'T APPLY WEIGHT DECAY)\n",
    "# TransformerEncoder: transformer_encoder_layers (DON'T APPLY WEIGHT DECAY TO RMSNORM), embedder (custom),pos_embed, norm_final (DON'T APPLY WEIGHT DECAY)\n",
    "\n",
    "# We don't want to apply weight decay to layer norms and embeddings\n",
    "no_weight_decay_params = [x for x in model.embedder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.decoder.transformer_decoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.decoder.norm_final.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.encoder.norm_final.parameters() if x.requires_grad]\n",
    "\n",
    "weight_decay_params = [x for name, x in model.decoder.transformer_decoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for x in model.decoder.command_encoder.parameters() if x.requires_grad]\n",
    "weight_decay_params += [x for x in model.decoder.command_decoder.parameters() if x.requires_grad]\n",
    "weight_decay_params += [x for x in model.encoder.embedder.parameters() if x.requires_grad]\n",
    "weight_decay_params += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for x in model.encoder.pos_embed.parameters() if x.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "       {'params': weight_decay_params, 'weight_decay': args['weight_decay']},\n",
    "       {'params': no_weight_decay_params, 'weight_decay': 0.0}\n",
    "    ],\n",
    "    betas=(0.9, 0.95),\n",
    "    lr=args['lr']\n",
    ")\n",
    "\n",
    "if args['use_scheduler']:\n",
    "    # scheduler = TransformerScheduler(\n",
    "    #     optimizer=optimizer,\n",
    "    #     dim_embed=args['embedding_dim'],\n",
    "    #     warmup_steps=args['scheduler_warmup_steps']\n",
    "    # )\n",
    "    batches_per_epoch = int(33928 * 26 / args['batch_size'] + 0.5)\n",
    "    scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args['epochs'] * batches_per_epoch, eta_min=1e-5)\n",
    "    scheduler2 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.001, end_factor=1.0, total_iters=args['scheduler_warmup_steps'])\n",
    "    scheduler = torch.optim.lr_scheduler.ChainedScheduler([scheduler1, scheduler2], optimizer=optimizer)\n",
    "\n",
    "dataset_name = \"basic-33928allchars_centered_scaled_sorted_filtered_cumulative_padded\"\n",
    "max_len = 33928\n",
    "num_glyphs = 26\n",
    "step_every = 1\n",
    "train_start, train_end = 0, int(0.95 * max_len) * num_glyphs\n",
    "test_start, test_end = train_end, max_len * num_glyphs\n",
    "# train_start, train_end = 0, 26*1\n",
    "# test_start, test_end = 0, 26*1\n",
    "cff_dataset = torch.load(f'./{dataset_name}.pt', mmap=True)[train_start:train_end:step_every]\n",
    "cff_dataset_test = torch.load(f'./{dataset_name}.pt', mmap=True)[test_start:test_end:step_every]\n",
    "im_dataset_name = \"basic-33928allchars_centered_scaled_sorted_filtered_(128, 128)\"\n",
    "im_dataset = torch.load(f'./{im_dataset_name}.pt', mmap=True)[train_start:train_end:step_every]\n",
    "im_dataset_test = torch.load(f'./{im_dataset_name}.pt', mmap=True)[test_start:test_end:step_every]\n",
    "cff_train_tensor_dataset = TensorDataset(cff_dataset, im_dataset)\n",
    "cff_train_dataloader = DataLoader(cff_train_tensor_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "cff_test_tensor_dataset = TensorDataset(cff_dataset_test, im_dataset_test)\n",
    "cff_test_dataloader = DataLoader(cff_test_tensor_dataset, batch_size=args['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['use_wandb']:\n",
    "    wandb.init(\n",
    "        project=\"project-typeface\",\n",
    "        config={\n",
    "            \"model_type\": \"Autoregressive CFF\",\n",
    "            **args\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(\n",
    "    reduction='sum',\n",
    "    ignore_index=tokenizer[pad_token],\n",
    "    label_smoothing=args['label_smoothing']\n",
    ")\n",
    "test_loss_fn = torch.nn.CrossEntropyLoss(\n",
    "    reduction='sum',\n",
    "    ignore_index=tokenizer[pad_token],\n",
    "    label_smoothing=0.0\n",
    ")\n",
    "def recon_loss(a, b):\n",
    "    return torch.pow((a - b), 2).sum()\n",
    "\n",
    "def kl_loss_fn(mu, logvar):\n",
    "    return 0.5 * ((torch.pow(mu, 2) + logvar.exp() - logvar - 1)).sum()\n",
    "\n",
    "curve_width = 7\n",
    "first_numeric_idx = 3 + len(tokenizer.possible_operators)\n",
    "zero_mask = (torch.ones((1,1,1,vocab_size)) * (torch.arange(0,vocab_size, dtype=torch.int32) >= first_numeric_idx)).to(device)\n",
    "hlf = curve_width // 2\n",
    "offset = torch.arange(-hlf, hlf+1, dtype=torch.int32)[None,None,:].to(device) # (1, 1, curve_width)\n",
    "neg_offset_exp = (-offset.abs().unsqueeze(-1)).exp()\n",
    "kl_loss = torch.nn.KLDivLoss(reduction='sum')\n",
    "cross_entropy_loss = torch.nn.CrossEntropyLoss(\n",
    "    reduction='sum',\n",
    "    ignore_index=tokenizer[tokenizer.pad_token],\n",
    "    label_smoothing=args['label_smoothing']\n",
    ")\n",
    "\n",
    "def numeric_mse_loss(output : torch.Tensor, targets : torch.Tensor):\n",
    "    # targets : (batch_size, seq_len)\n",
    "    with torch.no_grad():\n",
    "        is_numeric_tgt = (targets >= first_numeric_idx).unsqueeze(-1) # (batch_size, seq_len, 1)\n",
    "        is_non_padding = (targets > 0).unsqueeze(-1) # (batch_size, seq_len, 1)\n",
    "        # TODO: try removing the calculations from GPU and putting on CPU\n",
    "        token_count = targets.shape[0] * targets.shape[1]\n",
    "        arrng = torch.arange(0, token_count, dtype=torch.int32)\n",
    "        batch_indices = torch.floor_divide(arrng, targets.shape[1]).unsqueeze(0).to(device)\n",
    "        sequence_indices = torch.remainder(arrng, targets.shape[1]).unsqueeze(0).to(device)\n",
    "        token_indices = targets.flatten().unsqueeze(0)\n",
    "        single_tgt = torch.sparse_coo_tensor(\n",
    "            indices=torch.cat([batch_indices, sequence_indices, token_indices], dim=0),\n",
    "            values=torch.ones(token_count,).to(device),\n",
    "            size=(targets.shape[0], targets.shape[1], tokenizer.num_tokens)\n",
    "        ) # (batch_size, seq_len, vocab_size)\n",
    "        # single_tgt = torch.nn.functional.one_hot(targets.long(), tokenizer.num_tokens).int() # (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        token_count = targets.shape[0] * targets.shape[1] * curve_width\n",
    "        arrng = torch.arange(0, token_count, dtype=torch.int32)\n",
    "        batch_indices = torch.floor_divide(arrng, targets.shape[1] * curve_width).unsqueeze(0).to(device)\n",
    "        sequence_indices = torch.floor_divide(torch.remainder(arrng, targets.shape[1] * curve_width), curve_width).unsqueeze(0).to(device)\n",
    "        curve_indices = torch.remainder(arrng, curve_width).unsqueeze(0).to(device)\n",
    "        token_indices = torch.clip(targets.unsqueeze(-1) + offset, min=1, max=tokenizer.num_tokens-1).flatten().unsqueeze(0)\n",
    "        multi_tgt = torch.sparse_coo_tensor(\n",
    "            indices=torch.cat([batch_indices, sequence_indices, curve_indices, token_indices], dim=0),\n",
    "            values=torch.ones(token_count,).to(device),\n",
    "            size=(targets.shape[0], targets.shape[1], curve_width, tokenizer.num_tokens)\n",
    "        ) # (batch_size, seq_len, curve_width, vocab_size)\n",
    "        multi_tgt = multi_tgt * neg_offset_exp * zero_mask\n",
    "        # multi_tgt = torch.clip(targets.unsqueeze(-1) + offset, min=1, max=tokenizer.num_tokens-1) # (batch_size, seq_len, curve_width)\n",
    "        # multi_tgt = torch.nn.functional.one_hot(multi_tgt.long(), tokenizer.num_tokens).int() * neg_offset_exp # (batch_size, seq_len, curve_width, vocab_size)\n",
    "        # multi_tgt[:,:,:,:first_numeric_idx] = 0\n",
    "        multi_tgt = (multi_tgt.sum(dim=-2).to_dense() / (1e-10 + multi_tgt.sum(dim=(-2,-1)).unsqueeze(-1).to_dense())).to_sparse() # (batch_size, seq_len, vocab_size)\n",
    "        tgt_dist = is_numeric_tgt * multi_tgt + (~is_numeric_tgt) * (is_non_padding) * single_tgt # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "    return kl_loss(output.log_softmax(dim=-1), tgt_dist.to_dense())# + cross_entropy_loss(output, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Decoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(epoch : int, batch_idx : int):\n",
    "    with open('./fontmakerai/.config.txt', 'r') as cf:\n",
    "        lines = cf.readlines()\n",
    "        if len(lines) != 7:\n",
    "            print(f\"Not decoding this iteration; .config.txt has wrong number of lines ({len(lines)})\")\n",
    "            return\n",
    "        else:\n",
    "            decode_instr = DecodeInstruction(\n",
    "                decode_type=DecodeType[lines[0].split(\"=\")[-1].split(\".\")[-1].strip()],\n",
    "                sampling_type=SamplingType[lines[1].split(\"=\")[-1].split(\".\")[-1].strip()],\n",
    "                max_seq_len=int(lines[2].split(\"=\")[-1].strip()),\n",
    "                k=int(lines[3].split(\"=\")[-1].strip()),\n",
    "                p=float(lines[4].split(\"=\")[-1].strip()),\n",
    "                temp=float(lines[5].split(\"=\")[-1].strip()),\n",
    "                beam_size=int(lines[6].split(\"=\")[-1].strip())\n",
    "            )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            flag = True\n",
    "            idx = np.random.randint(0, im_dataset_test.shape[0])\n",
    "            im = im_dataset_test[idx:idx+1].to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "            sequence = model.decode(im, None, decode_instr)[0].cpu().detach().numpy().flatten()\n",
    "            torch.cuda.empty_cache()\n",
    "            # sequence = cff_train_tensor_dataset[0:1][0][0].cpu().detach().numpy().flatten()#.to(device)\n",
    "            toks = [tokenizer.reverse_map(tk.item(), use_int=True) for tk in sequence[:-1]]\n",
    "\n",
    "            print(\"Before:\", toks)\n",
    "            with open(f\"./fontmakerai/training_images/{epoch+1}_{batch_idx+1}.txt\", 'w') as f:\n",
    "                j_str = '\\', \\''\n",
    "                f.write(f\"Before: ['{j_str.join([str(x) for x in toks])}']\\n\\n\")\n",
    "                f.write(f\"decode_instr:\")\n",
    "                for k, v in decode_instr.__dict__.items():\n",
    "                    f.write(f\"\\n\\t{k}={v}\")\n",
    "                f.write(\"\\n\")\n",
    "            toks = [tok for tok in toks if tok != '<PAD2>']\n",
    "            if cumulative:\n",
    "                toks = numbers_first(make_non_cumulative(toks, tokenizer), tokenizer, return_string=False)\n",
    "                # toks = make_non_cumulative(toks, tokenizer)\n",
    "            else:\n",
    "                toks = numbers_first(toks, tokenizer, return_string=False)\n",
    "                # toks = toks\n",
    "            print(\"After:\", toks)\n",
    "            viz = Visualizer(toks)\n",
    "            with open(f\"./fontmakerai/training_images/{epoch+1}_{batch_idx+1}.txt\", 'a', newline='\\n') as f:\n",
    "                j_str = '\\', \\''\n",
    "                f.write(f\"After: ['{j_str.join([str(x) for x in toks])}']\")\n",
    "            \n",
    "            im_pixel_size = (128, 128)\n",
    "            crop_factor = 1.5\n",
    "            boundaries = (int((im_pixel_size[0] * (crop_factor - 1)) // 2), int((im_pixel_size[1] * (crop_factor - 1)) // 2))\n",
    "            ppi = 100\n",
    "            im_size_inches = ((im_pixel_size[0] * crop_factor) / ppi, (im_pixel_size[1] * crop_factor) / ppi)\n",
    "            img_arr = viz.draw(\n",
    "                display=False,\n",
    "                filename=f\"./fontmakerai/training_images/{epoch+1}_{batch_idx+1}.png\",\n",
    "                return_image=True,\n",
    "                center=True\n",
    "            )[None,:,:,0]\n",
    "            \n",
    "            img_arr = wandb.Image(img_arr, caption=f\"epoch{epoch+1}_{batch_idx+1}.png\")\n",
    "        except Exception as e:\n",
    "            flag = False\n",
    "            print(f\"Could not generate visualization; generated output was not formatted correctly: {e.args[0]}\")\n",
    "    model.train()\n",
    "    \n",
    "    if flag:\n",
    "        return (wandb.Image(im[0].to(device=device, dtype=torch.float32).cpu().detach().numpy(), caption=f\"epoch{epoch+1}_{batch_idx+1}.png\"), img_arr)\n",
    "    else:\n",
    "        return (wandb.Image(im[0].to(device=device, dtype=torch.float32).cpu().detach().numpy(), caption=f\"epoch{epoch+1}_{batch_idx+1}.png\"), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VAE for operator compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nTraining model for operator compression...\\n\")\n",
    "\n",
    "# # Note: in training, padding is included in the loss. This can be removed later during inference.\n",
    "# if args['train_transformer']:\n",
    "#     op_vae_optimizer = torch.optim.AdamW(op_vae.parameters(), lr=args['vae_lr'], weight_decay=args['vae_weight_decay'])\n",
    "#     for epoch in range(args['vae_epochs']):\n",
    "#         op_vae.train()\n",
    "#         total_loss = 0\n",
    "#         for (X, _) in tqdm(cff_train_dataloader):\n",
    "#             inputs = X.to(device)\n",
    "            \n",
    "#             op_vae_optimizer.zero_grad()\n",
    "#             inp, inp_hat, mu, logvar = op_vae(inputs, tokenizer)\n",
    "#             loss = (numeric_mse_loss(inp_hat, inp) + args['vae_beta'] * kl_loss_fn(mu, logvar)) / inp.shape[0]\n",
    "#             total_loss += loss.item()\n",
    "#             loss.backward()\n",
    "#             op_vae_optimizer.step()\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         op_vae.eval()\n",
    "#         test_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for (X, _) in tqdm(cff_test_dataloader):\n",
    "#                 inputs = X.to(device)\n",
    "                \n",
    "#                 inp, inp_hat, mu, logvar = op_vae(inputs, tokenizer)\n",
    "#                 loss = (numeric_mse_loss(inp_hat, inp) + args['vae_beta'] * kl_loss_fn(mu, logvar)) / inp.shape[0]\n",
    "#                 test_loss += loss.item()\n",
    "\n",
    "#             test_val = torch.IntTensor([[9, 302, 305, 122, 955, 1002, 554, 36]]).to(device)\n",
    "#             inp, inp_hat, mu, logvar = op_vae(test_val, tokenizer)\n",
    "#             loss = (numeric_mse_loss(inp_hat, inp) + args['vae_beta'] * kl_loss_fn(mu, logvar))\n",
    "#             print(f\"loss: {loss}, mu: {mu.abs().mean().item()}, logvar: {logvar.abs().mean().item()}\")\n",
    "#             print(inp_hat.argmax(dim=1).cpu().detach().numpy().flatten().tolist())\n",
    "\n",
    "#             if args['use_wandb']:\n",
    "#                 wandb.log({\n",
    "#                     \"train_loss\": total_loss / cff_dataset.shape[0],\n",
    "#                     \"test_loss\": test_loss / cff_dataset_test.shape[0]\n",
    "#                 })\n",
    "#         print(f\"epoch {epoch+1}/{args['vae_epochs']}: train loss: {total_loss / cff_dataset.shape[0]}     test loss: {test_loss / cff_dataset_test.shape[0]}\")\n",
    "\n",
    "#     torch.save(op_vae, f'models/op_vae-{dataset_name}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining model...\\n\")\n",
    "\n",
    "if args['train_transformer']:\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_acc_list = []\n",
    "    src = torch.zeros((args['batch_size'], 0)).to(device)\n",
    "    for epoch in range(args['epochs']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_batches = (max_len*num_glyphs // args['batch_size']) + 1\n",
    "        for idx, (X, im) in enumerate(tqdm(cff_train_dataloader, total=train_batches)):\n",
    "            if idx >= train_batches:\n",
    "                break\n",
    "            inputs = X.to(device)\n",
    "            im = im.to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "            optimizer.zero_grad()\n",
    "            out = model(im, inputs[:,:-7]) # Use only output tokens before this truth term\n",
    "            # loss = loss_fn(out.permute(0, 2, 1), inputs.long())\n",
    "            loss = numeric_mse_loss(out, inputs)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            if args['gradient_clip']:\n",
    "                torch.nn.utils.clip_grad_value_(model.parameters(), args['gradient_clip_val'])\n",
    "            optimizer.step()\n",
    "            if args['use_scheduler']:\n",
    "                scheduler.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if args['use_wandb']:\n",
    "                if (idx+1) % 250 == 0:\n",
    "                    goal_image, img_arr = decode(epoch, idx)\n",
    "                    wandb.log({\n",
    "                        \"goal_image\": goal_image,\n",
    "                        \"images\": img_arr,\n",
    "                        \"train_loss_step\": loss.item() / inputs.shape[0],\n",
    "                        \"lr_step\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                    })\n",
    "                else:\n",
    "                    wandb.log({\n",
    "                        \"train_loss_step\": loss.item() / inputs.shape[0],\n",
    "                        \"lr_step\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                    })\n",
    "        # train_loss_list += [total_loss / cff_dataset.shape[0]]\n",
    "        train_loss_list += [total_loss / (min(train_batches, idx+1)*args['batch_size'])]\n",
    "        \n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        test_batches = 25\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_negatives = 0\n",
    "        with torch.no_grad():\n",
    "            for idx, (X, im) in enumerate(tqdm(cff_test_dataloader, total=test_batches)):\n",
    "                if idx >= test_batches:\n",
    "                    break\n",
    "                inputs = X.to(device)\n",
    "                im = im.to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "                out = model(im, inputs[:,:-7]) # Use only output tokens before this truth term\n",
    "                # loss = loss_fn(out.permute(0, 2, 1), inputs.long())\n",
    "                loss = numeric_mse_loss(out, inputs)\n",
    "                total_loss += loss.item()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                guesses = out.permute(0, 2, 1).argmax(dim=1)\n",
    "                truths = inputs\n",
    "                true_positives += ((guesses == truths) * (truths != tokenizer[pad_token])).sum()\n",
    "                false_positives += ((guesses != truths) * (truths == tokenizer[pad_token])).sum()\n",
    "                true_negatives += ((guesses == truths) * (truths == tokenizer[pad_token])).sum()\n",
    "                false_negatives += ((guesses != truths) * (truths != tokenizer[pad_token])).sum()\n",
    "            \n",
    "            # test_loss_list += [total_loss / cff_dataset_test.shape[0]]\n",
    "            test_loss_list += [total_loss / (min(test_batches, idx+1)*args['batch_size'])]\n",
    "            acc, pre, rec, f1 = PerformanceMetrics.all_metrics(\n",
    "                tp=true_positives,\n",
    "                fp=false_positives,\n",
    "                tn=true_negatives,\n",
    "                fn=false_negatives\n",
    "            )\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{args['epochs']} completed. Train Loss = {train_loss_list[-1]};  Test Loss: {test_loss_list[-1]}\")\n",
    "            # torch.save(model, './fontmakerai/model.pkl')\n",
    "            \n",
    "            if (epoch + 1) % args['sample_every'] == 0:\n",
    "                with open('./fontmakerai/.config.txt', 'r') as cf:\n",
    "                    lines = cf.readlines()\n",
    "                    if len(lines) != 7:\n",
    "                        print(f\"Not decoding this iteration; .config.txt has wrong number of lines ({len(lines)})\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        decode_instr = DecodeInstruction(\n",
    "                            decode_type=DecodeType[lines[0].split(\"=\")[-1].split(\".\")[-1].strip()],\n",
    "                            sampling_type=SamplingType[lines[1].split(\"=\")[-1].split(\".\")[-1].strip()],\n",
    "                            max_seq_len=int(lines[2].split(\"=\")[-1].strip()),\n",
    "                            k=int(lines[3].split(\"=\")[-1].strip()),\n",
    "                            p=float(lines[4].split(\"=\")[-1].strip()),\n",
    "                            temp=float(lines[5].split(\"=\")[-1].strip()),\n",
    "                            beam_size=int(lines[6].split(\"=\")[-1].strip())\n",
    "                        )\n",
    "\n",
    "                try:\n",
    "                    flag = True\n",
    "                    idx = np.random.randint(0, im_dataset_test.shape[0])\n",
    "                    im = im_dataset_test[idx:idx+1].to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "                    sequence = model.decode(im, None, decode_instr)[0].cpu().detach().numpy().flatten()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    # sequence = cff_train_tensor_dataset[0:1][0][0].cpu().detach().numpy().flatten()#.to(device)\n",
    "                    toks = [tokenizer.reverse_map(tk.item(), use_int=True) for tk in sequence[:-1]]\n",
    "\n",
    "                    print(\"Before:\", toks)\n",
    "                    with open(f\"./fontmakerai/training_images/{epoch+1}.txt\", 'w') as f:\n",
    "                        j_str = '\\', \\''\n",
    "                        f.write(f\"Before: ['{j_str.join([str(x) for x in toks])}']\\n\\n\")\n",
    "                        f.write(f\"decode_instr:\")\n",
    "                        for k, v in decode_instr.__dict__.items():\n",
    "                            f.write(f\"\\n\\t{k}={v}\")\n",
    "                        f.write(\"\\n\")\n",
    "                    toks = [tok for tok in toks if tok != '<PAD2>']\n",
    "                    if cumulative:\n",
    "                        toks = numbers_first(make_non_cumulative(toks, tokenizer), tokenizer, return_string=False)\n",
    "                        # toks = make_non_cumulative(toks, tokenizer)\n",
    "                    else:\n",
    "                        toks = numbers_first(toks, tokenizer, return_string=False)\n",
    "                        # toks = toks\n",
    "                    print(\"After:\", toks)\n",
    "                    viz = Visualizer(toks)\n",
    "                    with open(f\"./fontmakerai/training_images/{epoch+1}.txt\", 'a', newline='\\n') as f:\n",
    "                        j_str = '\\', \\''\n",
    "                        f.write(f\"After: ['{j_str.join([str(x) for x in toks])}']\")\n",
    "                    \n",
    "                    im_pixel_size = (128, 128)\n",
    "                    crop_factor = 1.5\n",
    "                    boundaries = (int((im_pixel_size[0] * (crop_factor - 1)) // 2), int((im_pixel_size[1] * (crop_factor - 1)) // 2))\n",
    "                    ppi = 100\n",
    "                    im_size_inches = ((im_pixel_size[0] * crop_factor) / ppi, (im_pixel_size[1] * crop_factor) / ppi)\n",
    "                    img_arr = viz.draw(\n",
    "                        display=False,\n",
    "                        filename=f\"./fontmakerai/training_images/{epoch+1}.png\",\n",
    "                        return_image=True,\n",
    "                        center=True\n",
    "                    )[None,:,:,0]\n",
    "                    \n",
    "                    img_arr = wandb.Image(img_arr, caption=f\"epoch{epoch+1}.png\")\n",
    "                    # wandb.log({\"images\": img_arr}) # TODO: also log decoder instructions\n",
    "                except Exception as e:\n",
    "                    flag = False\n",
    "                    print(f\"Could not generate visualization; generated output was not formatted correctly: {e.args[0]}\")\n",
    "\n",
    "            \n",
    "            if (epoch + 1) % args['sample_every'] == 0 and flag:\n",
    "                if args['use_wandb']:\n",
    "                    wandb.log({\n",
    "                        \"train_loss\": train_loss_list[-1],\n",
    "                        \"test_loss\": test_loss_list[-1],\n",
    "                        \"test_accuracy\": acc,\n",
    "                        \"test_precision\": pre,\n",
    "                        \"test_recall\": rec,\n",
    "                        \"test_f1\": f1,\n",
    "                        \"lr\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                        \"goal_image\": wandb.Image(im[0].to(device=device, dtype=torch.float32).cpu().detach().numpy(), caption=f\"epoch{epoch+1}.png\"),\n",
    "                        \"images\": img_arr\n",
    "                    })\n",
    "            elif (epoch + 1) % args['sample_every'] == 0:\n",
    "                if args['use_wandb']:\n",
    "                    wandb.log({\n",
    "                        \"train_loss\": train_loss_list[-1],\n",
    "                        \"test_loss\": test_loss_list[-1],\n",
    "                        \"test_accuracy\": acc,\n",
    "                        \"test_precision\": pre,\n",
    "                        \"test_recall\": rec,\n",
    "                        \"test_f1\": f1,\n",
    "                        \"lr\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                        \"goal_image\": wandb.Image(im[0].to(device=device, dtype=torch.float32).cpu().detach().numpy(), caption=f\"epoch{epoch+1}.png\"),\n",
    "                    })\n",
    "            else:\n",
    "                if args['use_wandb']:\n",
    "                    wandb.log({\n",
    "                        \"train_loss\": train_loss_list[-1],\n",
    "                        \"test_loss\": test_loss_list[-1],\n",
    "                        \"test_accuracy\": acc,\n",
    "                        \"test_precision\": pre,\n",
    "                        \"test_recall\": rec,\n",
    "                        \"test_f1\": f1,\n",
    "                        \"lr\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                    })\n",
    "\n",
    "        if (epoch+1) % 100 == 0 or epoch+1 == args['epochs']:\n",
    "            torch.save(model, f'models/transformer-{dataset_name}-{epoch+1}.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
