{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ec2-user/FontMakerAI/backend')\n",
    "\n",
    "from config import device, operators, DecodeType, DecodeInstruction, SamplingType\n",
    "from ml.tokenizer import Tokenizer\n",
    "from ml.fontmodel import DecodeInstruction, FontModel\n",
    "from ml.performance import PerformanceMetrics\n",
    "from parsing.glyph_viz import Visualizer\n",
    "from parsing.tablelist_utils import numbers_first, make_non_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing train-cff.ipynb on cuda...\n",
      "-----------------------------\n",
      "Posttraining hyperparameters:\n",
      "{'batch_accumulate': 1,\n",
      " 'batch_size': 8,\n",
      " 'data_type': torch.bfloat16,\n",
      " 'dropout_rate': 0.2,\n",
      " 'embedding_dim': 1024,\n",
      " 'epochs': 1,\n",
      " 'ff_dim': 4096,\n",
      " 'freeze_embeddings': False,\n",
      " 'gradient_clip': True,\n",
      " 'gradient_clip_val': 1.0,\n",
      " 'label_smoothing': 0.001,\n",
      " 'load_model': True,\n",
      " 'lr': 0.0003,\n",
      " 'max_number': 500,\n",
      " 'max_seq_len': 5040,\n",
      " 'min_number': -500,\n",
      " 'num_heads': 16,\n",
      " 'num_layers': 12,\n",
      " 'pretrain_batch_size': 128,\n",
      " 'pretrain_embeddings': False,\n",
      " 'pretrain_epochs': 1,\n",
      " 'pretrain_lr': 0.004,\n",
      " 'pretrain_scheduler_warmup_steps': 3000,\n",
      " 'pretrain_use_scheduler': True,\n",
      " 'sample_every': 1,\n",
      " 'scheduler_warmup_steps': 2000,\n",
      " 'train_transformer': True,\n",
      " 'use_pretrained_embeddings': False,\n",
      " 'use_pretrained_vit_encoder': False,\n",
      " 'use_scheduler': False,\n",
      " 'use_wandb': True,\n",
      " 'vae_beta': 0.1,\n",
      " 'vae_epochs': 10,\n",
      " 'vae_lr': 0.01,\n",
      " 'vae_weight_decay': 1e-05,\n",
      " 'weight_decay': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Executing train-cff.ipynb on {device}...\\n-----------------------------\")\n",
    "\n",
    "args = {\n",
    "    \"load_model\": True,\n",
    "    \"train_transformer\": True,\n",
    "    \"min_number\": -500,\n",
    "    \"max_number\": 500,\n",
    "    \"max_seq_len\": 5040,\n",
    "    \"num_layers\": 12,\n",
    "    \"embedding_dim\": 1024,\n",
    "    \"num_heads\": 16,\n",
    "    \"ff_dim\": 4096,\n",
    "    \"use_wandb\": True,\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"batch_accumulate\": 1,\n",
    "    \"lr\": 3e-4,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"weight_decay\": 1e-1,\n",
    "    \"gradient_clip\": True,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"label_smoothing\": 0.001,\n",
    "    \"sample_every\": 1,\n",
    "    \"use_scheduler\": False,\n",
    "    \"scheduler_warmup_steps\": 2000,\n",
    "    \"data_type\": torch.bfloat16,\n",
    "    \"vae_beta\": 1e-1,\n",
    "    \"vae_epochs\": 10,\n",
    "    \"vae_lr\": 1e-2,\n",
    "    \"vae_weight_decay\": 1e-5,\n",
    "    \"freeze_embeddings\": False,\n",
    "    \"use_pretrained_embeddings\": False,\n",
    "    \"pretrain_embeddings\": False,\n",
    "    \"pretrain_epochs\": 1,\n",
    "    \"pretrain_batch_size\": 128,\n",
    "    \"pretrain_lr\": 4e-3,\n",
    "    \"pretrain_use_scheduler\": True,\n",
    "    \"pretrain_scheduler_warmup_steps\": 3000,\n",
    "    \"use_pretrained_vit_encoder\": False,\n",
    "}\n",
    "\n",
    "print(\"Posttraining hyperparameters:\")\n",
    "pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = \"<PAD>\"\n",
    "sos_token = \"<SOS>\"\n",
    "eos_token = \"<EOS>\"\n",
    "tokenizer = Tokenizer(\n",
    "    min_number=args['min_number'],\n",
    "    max_number=args['max_number'],\n",
    "    possible_operators=operators,\n",
    "    pad_token=pad_token,\n",
    "    sos_token=sos_token,\n",
    "    eos_token=eos_token\n",
    ")\n",
    "cumulative = True\n",
    "vocab_size = tokenizer.num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_instr = DecodeInstruction( # NOTE: doesn't matter unless loading from .config.txt fails\n",
    "    DecodeType.ANCESTRAL,\n",
    "    SamplingType.TEMPERATURE,\n",
    "    max_seq_len=args['max_seq_len'],\n",
    "    k=3,\n",
    "    p=0,\n",
    "    temp=1.0,\n",
    "    beam_size=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_folder = f'../../../models'\n",
    "if args['load_model']:\n",
    "    model_pre = torch.load(f'{models_folder}/transformer-basic-33928allchars_centered_scaled_sorted_filtered_cumulative_padded-14.pkl', map_location=device, weights_only=False).to(device)\n",
    "else:\n",
    "    model_pre = FontModel(\n",
    "        num_enc_layers=args['num_layers'],\n",
    "        num_dec_layers=args['num_layers'],\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=args['embedding_dim'],\n",
    "        num_heads=args['num_heads'],\n",
    "        ff_dim=args['ff_dim'],\n",
    "        dropout_rate=args['dropout_rate'],\n",
    "        max_seq_len=args['max_seq_len'],\n",
    "        device=device\n",
    "    ).to(device, dtype=args['data_type'])\n",
    "# model = torch.compile(model_pre)\n",
    "model = deepcopy(model_pre)\n",
    "original_model = deepcopy(model)\n",
    "original_model.eval()\n",
    "\n",
    "del model_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (tentative):\n",
    "# FontModel: embedder (DON'T APPLY WEIGHT DECAY)\n",
    "# TransformerDecoder: transformer_decoder_layers (DON'T APPLY WEIGHT DECAY TO RMSNORM), command_encoder, command_decoder, norm_final (DON'T APPLY WEIGHT DECAY)\n",
    "# TransformerEncoder: transformer_encoder_layers (DON'T APPLY WEIGHT DECAY TO RMSNORM), embedder (custom),pos_embed, norm_final (DON'T APPLY WEIGHT DECAY)\n",
    "\n",
    "# We don't want to apply weight decay to layer norms and embeddings\n",
    "no_weight_decay_params = [x for x in model.decoder.embedder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.inverse_embedder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.decoder.transformer_decoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.decoder.norm_final.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.encoder.norm_final.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_encoder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_decoder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_decoder_2a.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_decoder_2b.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.command_decoder_1.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.command_decoder_2.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.W_cn.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.W_cnb.parameters() if x.requires_grad]\n",
    "\n",
    "weight_decay_params = [x for name, x in model.decoder.transformer_decoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for x in model.encoder.pos_embed.parameters() if x.requires_grad]\n",
    "\n",
    "vit_encoder_params_nwd = [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad]# and ('norm' in name or 'bias' in name)]\n",
    "# vit_encoder_params_nwd += [x for name, x in model.encoder.pretrain_reverse_ae.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "vit_encoder_params_nwd += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "vit_encoder_params_nwd += [x for x in model.encoder.norm_final.parameters() if x.requires_grad]\n",
    "# vit_encoder_params_wd = [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "vit_encoder_params_nwd += [x for name, x in model.encoder.pretrain_reverse_ae.named_parameters() if x.requires_grad]# and 'norm' not in name and 'bias' not in name]\n",
    "vit_encoder_params_wd = [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "vit_encoder_params_wd += [x for x in model.encoder.pos_embed.parameters() if x.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "       {'params': weight_decay_params, 'weight_decay': args['weight_decay']},\n",
    "       {'params': no_weight_decay_params, 'weight_decay': args['weight_decay']}\n",
    "    ],\n",
    "    betas=(0.9, 0.95),\n",
    "    lr=args['lr'] \n",
    ")\n",
    "\n",
    "max_len = 33928\n",
    "num_glyphs = 26\n",
    "step_every = 1\n",
    "\n",
    "if args['use_scheduler']:\n",
    "    # scheduler = TransformerScheduler(\n",
    "    #     optimizer=optimizer,\n",
    "    #     dim_embed=args['embedding_dim'],\n",
    "    #     warmup_steps=args['scheduler_warmup_steps']\n",
    "    # )\n",
    "    batches_per_epoch = int(max_len * (num_glyphs // step_every) / args['batch_size'] + 0.5)\n",
    "    scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args['epochs'] * (batches_per_epoch // args['batch_accumulate']), eta_min=1e-5)\n",
    "    scheduler2 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.001, end_factor=1.0, total_iters=args['scheduler_warmup_steps'])\n",
    "    scheduler = torch.optim.lr_scheduler.ChainedScheduler([scheduler1, scheduler2], optimizer=optimizer)\n",
    "\n",
    "dataset_dir = \"../../..\"\n",
    "dataset_name = f\"{dataset_dir}/basic-33928allchars_centered_scaled_sorted_filtered{'_cumulative' if cumulative else ''}_padded\"\n",
    "train_start, train_end = 0, int(0.95 * max_len) * num_glyphs\n",
    "test_start, test_end = train_end, max_len * num_glyphs\n",
    "# max_len = 5\n",
    "# train_start, train_end = 0, 26*max_len\n",
    "# test_start, test_end = 0, 26*max_len\n",
    "cff_dataset = torch.load(f'./{dataset_name}.pt', mmap=True)[train_start:train_end:step_every]\n",
    "cff_dataset_test = torch.load(f'./{dataset_name}.pt', mmap=True)[test_start:test_end:step_every]\n",
    "im_dataset_name = f\"{dataset_dir}/basic-33928allchars_centered_scaled_sorted_filtered_(128, 128)\"\n",
    "im_dataset = torch.load(f'./{im_dataset_name}.pt', mmap=True)[train_start:train_end:step_every]\n",
    "im_dataset_test = torch.load(f'./{im_dataset_name}.pt', mmap=True)[test_start:test_end:step_every]\n",
    "cff_train_tensor_dataset = TensorDataset(cff_dataset, im_dataset)\n",
    "cff_train_dataloader = DataLoader(cff_train_tensor_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "cff_test_tensor_dataset = TensorDataset(cff_dataset_test, im_dataset_test)\n",
    "cff_test_dataloader = DataLoader(cff_test_tensor_dataset, batch_size=args['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-training model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPost-training model...\\n\")\n",
    "\n",
    "kl_loss = torch.nn.functional.kl_div\n",
    "\n",
    "@torch.no_grad()\n",
    "def value_fn(image_gt, output_tokens):\n",
    "    '''\n",
    "    image_gt: the ground truth image\n",
    "    output_tokens: the model's predicted output\n",
    "    '''\n",
    "    try:\n",
    "        sequence = output_tokens.cpu().detach().numpy().flatten()\n",
    "        toks = [tokenizer.reverse_map(tk.item(), use_int=True) for tk in sequence]\n",
    "        toks = [tok for tok in toks if tok != '<PAD2>' and tok != '<PAD>']\n",
    "        if cumulative:\n",
    "            toks = numbers_first(make_non_cumulative(toks, tokenizer), tokenizer, return_string=False)\n",
    "        else:\n",
    "            toks = numbers_first(toks, tokenizer, return_string=False)\n",
    "        viz = Visualizer(toks)\n",
    "        im_pixel_size = (128, 128)\n",
    "        crop_factor = 1\n",
    "        dpi = 1\n",
    "        boundaries = (int((im_pixel_size[0] * (crop_factor * 100 / dpi - 1)) // 2), int((im_pixel_size[1] * (crop_factor * 100 / dpi - 1)) // 2))\n",
    "        im_size_inches = ((im_pixel_size[0] * crop_factor) / dpi, (im_pixel_size[1] * crop_factor) / dpi)\n",
    "        output_image = viz.draw(\n",
    "            display=False,\n",
    "            filename=None,\n",
    "            return_image=True,\n",
    "            center=False,\n",
    "            im_size_inches=im_size_inches,\n",
    "            bounds=(-300, 300),\n",
    "            dpi=dpi\n",
    "        )[:,:,0] / 255.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error drawing image: {e} for tokens {toks}\")\n",
    "        output_image = np.zeros((128, 128))\n",
    "    value = -torch.abs((image_gt + 1) / 2 - torch.Tensor(output_image).to(device)).mean()\n",
    "    return value\n",
    "\n",
    "@torch.no_grad()\n",
    "def advantage_fn(image_gt, output_tokens):\n",
    "    '''\n",
    "    The value of the current output tokens (\"next state\") minus the value of the previous output tokens (\"current state\")\n",
    "    \n",
    "    image_gt: the ground truth image\n",
    "    output_tokens: the model's predicted output\n",
    "    '''\n",
    "    return value_fn(image_gt, output_tokens) - value_fn(image_gt, output_tokens[:,:-7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(epoch : int, batch_idx : int):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            flag = True\n",
    "            idx = np.random.randint(0, im_dataset_test.shape[0])\n",
    "            im = im_dataset_test[idx:idx+1].to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "            sequence = model.decode(im, None, decode_instr)[0].cpu().detach().numpy().flatten()\n",
    "            if len(sequence) == decode_instr.max_seq_len:\n",
    "                toks = [tokenizer.reverse_map(tk.item(), use_int=True) for tk in sequence] + ['endchar']\n",
    "            else:\n",
    "                toks = [tokenizer.reverse_map(tk.item(), use_int=True) for tk in sequence[:-1]]\n",
    "\n",
    "            print(\"Before:\", toks)\n",
    "            toks = [tok for tok in toks if tok != '<PAD2>' and tok != '<PAD>']\n",
    "            if cumulative:\n",
    "                toks = numbers_first(make_non_cumulative(toks, tokenizer), tokenizer, return_string=False)\n",
    "                # toks = make_non_cumulative(toks, tokenizer)\n",
    "            else:\n",
    "                toks = numbers_first(toks, tokenizer, return_string=False)\n",
    "                # toks = toks\n",
    "            print(\"After:\", toks)\n",
    "            viz = Visualizer(toks)\n",
    "\n",
    "            if toks[2] != \"rmoveto\" and toks[3] != \"rmoveto\":\n",
    "                raise Exception(\"first operator is not rmoveto\")\n",
    "            \n",
    "            im_pixel_size = (128, 128)\n",
    "            crop_factor = 1\n",
    "            dpi = 1\n",
    "            boundaries = (int((im_pixel_size[0] * (crop_factor * 100 / dpi - 1)) // 2), int((im_pixel_size[1] * (crop_factor * 100 / dpi - 1)) // 2))\n",
    "            im_size_inches = ((im_pixel_size[0] * crop_factor) / dpi, (im_pixel_size[1] * crop_factor) / dpi)\n",
    "            img_arr = viz.draw(\n",
    "                display=False,\n",
    "                filename=None,\n",
    "                return_image=True,\n",
    "                center=False,\n",
    "                im_size_inches=im_size_inches,\n",
    "                bounds=(-300, 300),\n",
    "                dpi=dpi\n",
    "            )[None,:,:,0]\n",
    "            \n",
    "            im_cpu = (im[0] * 127.5 + 127.5).to(device=device, dtype=torch.uint8).cpu().detach().numpy()\n",
    "            img_arr = wandb.Image(np.concatenate([im_cpu, img_arr], axis=2), caption=f\"epoch{epoch+1}_{batch_idx+1}.png\")\n",
    "        except Exception as e:\n",
    "            flag = False\n",
    "            print(f\"Could not generate visualization; generated output was not formatted correctly: {e}\")\n",
    "    model.train()\n",
    "    \n",
    "    if flag:\n",
    "        return (wandb.Image(im[0].to(device=device, dtype=torch.float32).cpu().detach().numpy(), caption=f\"epoch{epoch+1}_{batch_idx+1}.png\"), img_arr)\n",
    "    else:\n",
    "        print(\"Decoding failed.\")\n",
    "        return (None, None)\n",
    "        # return (wandb.Image(im[0].to(device=device, dtype=torch.float32).cpu().detach().numpy(), caption=f\"epoch{epoch+1}_{batch_idx+1}.png\"), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['use_wandb']:\n",
    "    wandb.init(\n",
    "        project=\"project-typeface\",\n",
    "        config={\n",
    "            \"model_type\": \"Autoregressive CFF\",\n",
    "            **args\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 17\u001b[0m     out_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrl_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_instr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m decode_instr\u001b[38;5;241m.\u001b[39mmax_seq_len:\n",
      "File \u001b[0;32m~/FontMakerAI/backend/ml/fontmodel.py:900\u001b[0m, in \u001b[0;36mFontModel.rl_decode\u001b[0;34m(self, src, tgt, instruction)\u001b[0m\n\u001b[1;32m    898\u001b[0m         tgt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((src\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mto(src\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    899\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src)\n\u001b[0;32m--> 900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrl_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FontMakerAI/backend/ml/fontmodel.py:806\u001b[0m, in \u001b[0;36mTransformerDecoder.rl_decode\u001b[0;34m(self, x, tgt, instruction)\u001b[0m\n\u001b[1;32m    803\u001b[0m seq, new_kv_caches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step(x, tgt, instruction, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, new_kv_caches)\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m seq\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m instruction\u001b[38;5;241m.\u001b[39mmax_seq_len:\n\u001b[0;32m--> 806\u001b[0m     seq, new_kv_caches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_kv_caches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos_token[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m seq[b_idx,:]:\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;66;03m# continue\u001b[39;00m\n",
      "File \u001b[0;32m~/FontMakerAI/backend/ml/fontmodel.py:519\u001b[0m, in \u001b[0;36mTransformerDecoder._step\u001b[0;34m(self, x, tgt, instruction, scores, continue_samples, src_mask, kv_caches)\u001b[0m\n\u001b[1;32m    515\u001b[0m select_last \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instruction\u001b[38;5;241m.\u001b[39msampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mMULTINOMIAL:\n\u001b[1;32m    518\u001b[0m     nxt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(\n\u001b[0;32m--> 519\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mdecoder_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mselect_last\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mselect_last\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    520\u001b[0m         num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    521\u001b[0m         replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     )\u001b[38;5;241m.\u001b[39mview(B, select_last, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Default ancestral\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m instruction\u001b[38;5;241m.\u001b[39msampling_type \u001b[38;5;241m==\u001b[39m SamplingType\u001b[38;5;241m.\u001b[39mTEMPERATURE:\n\u001b[1;32m    525\u001b[0m     nxt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m(decoder_out[:,\u001b[38;5;241m-\u001b[39mselect_last:,:] \u001b[38;5;241m/\u001b[39m instruction\u001b[38;5;241m.\u001b[39mtemp)\u001b[38;5;241m.\u001b[39mview(B \u001b[38;5;241m*\u001b[39m select_last, d)\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    527\u001b[0m         num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    528\u001b[0m         replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    529\u001b[0m     )\u001b[38;5;241m.\u001b[39mview(B, select_last, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "src = torch.zeros((args['batch_size'], 0)).to(device)\n",
    "for epoch in range(args['epochs']):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    last_loss = 0\n",
    "    train_batches = (max_len*(num_glyphs // step_every) // args['batch_size']) + 1\n",
    "    for idx, (X, im) in enumerate((cff_train_dataloader)):\n",
    "        if idx >= train_batches:\n",
    "            break\n",
    "        inputs = X.to(device, dtype=torch.int32)\n",
    "        im = im.to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "\n",
    "        # output tokens from the current model\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            out_tokens = model.rl_decode(im, None, decode_instr)\n",
    "            model.train()\n",
    "        if out_tokens.shape[1] != decode_instr.max_seq_len:\n",
    "            out_tokens = out_tokens[:,:-1]\n",
    "\n",
    "        print(out_tokens.shape)\n",
    "\n",
    "        # ADVANTAGES\n",
    "        gamma_ = 0.99\n",
    "        lambda_ = 0.95\n",
    "        values = torch.zeros((out_tokens.shape[0], out_tokens.shape[1]//7-1)).to(device, dtype=args['data_type'])\n",
    "        for b in range(out_tokens.shape[0]):\n",
    "            index_of_endchar = (out_tokens[b,:] != model.decoder.pad_token[0]).nonzero(as_tuple=True)[0][-1]\n",
    "            for i in tqdm(range(2, index_of_endchar//7+1)):\n",
    "                values[b,i-2] = value_fn(im[b], out_tokens[b,:i*7])\n",
    "        deltas = torch.Tensor([[gamma_ * values[b][i+1] - values[b][i] for i in range(values.shape[1]-1)] for b in range(out_tokens.shape[0])]).to(device, dtype=args['data_type']) # (batch_size, seq_len=5040)\n",
    "        adv = torch.zeros_like(deltas)\n",
    "        for i in range(adv.shape[1]):\n",
    "            if i == 0:\n",
    "                adv[:,adv.shape[1]-i-1] = deltas[:,adv.shape[1]-i-1]\n",
    "            else:\n",
    "                adv[:,adv.shape[1]-i-1] = deltas[:,adv.shape[1]-i-1] + gamma_ * lambda_ * adv[:,adv.shape[1]-i]\n",
    "        adv = adv.repeat_interleave(7, dim=1) # (batch_size, seq_len=5040)\n",
    "\n",
    "\n",
    "        \n",
    "        # token distributions\n",
    "        in_tokens = out_tokens[:,:-7] # note: SOS token is prepended in forward()\n",
    "        print(\"getting dist_new...\")\n",
    "        dist_new = model(im, in_tokens)\n",
    "        print(\"got dist_new \", dist_new.shape)\n",
    "        print(\"getting dist_original...\")\n",
    "        with torch.no_grad():\n",
    "            dist_original = original_model(im, in_tokens)\n",
    "        print(\"got dist_original \", dist_original.shape)\n",
    "        \n",
    "        prob_new = torch.gather(dist_new, dim=-1, index=out_tokens.unsqueeze(1)).squeeze(1)\n",
    "        prob_original = torch.gather(dist_original, dim=-1, index=out_tokens.unsqueeze(1)).squeeze(1)\n",
    "        rel_prob = prob_new / prob_original # (batch_size=1, seq_len)\n",
    "        eps = 0.2\n",
    "        loss = -(torch.minimum(rel_prob[:,14:] * adv, torch.clip(rel_prob[:,14:], 1-eps, 1+eps) * adv) * (out_tokens[:,14:] != tokenizer[pad_token])).mean()\n",
    "\n",
    "        print(\"propagating loss...\")\n",
    "\n",
    "        total_loss += loss.item() * X.shape[0]\n",
    "        loss.backward()\n",
    "\n",
    "        if (idx+1) % 1 == 0 or idx == train_batches-1:\n",
    "            if args['gradient_clip']:\n",
    "                torch.nn.utils.clip_grad_value_(model.parameters(), args['gradient_clip_val'])\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if args['use_scheduler']:\n",
    "                scheduler.step()\n",
    "            diff = total_loss - last_loss\n",
    "            last_loss = total_loss\n",
    "\n",
    "        if args['use_wandb']:\n",
    "            if (idx+1) % 2 == 0 or (idx == train_batches-1 and (epoch+1) % args['sample_every'] == 0):\n",
    "                goal_image, img_arr = decode(epoch, idx)\n",
    "                wandb.log({\n",
    "                    \"posttrain_goal_image\": goal_image,\n",
    "                    \"posttrain_images\": img_arr,\n",
    "                    \"posttrain_loss_step\": diff / (args['batch_accumulate'] * args['batch_size']),\n",
    "                    \"posttrain_lr_step\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                })\n",
    "            elif (idx+1) % 1 == 0:\n",
    "                wandb.log({\n",
    "                    \"posttrain_loss_step\": diff / (args['batch_accumulate'] * args['batch_size']),\n",
    "                    \"posttrain_lr_step\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                })\n",
    "    train_loss = total_loss / (min(train_batches, idx+1)*args['batch_size'])\n",
    "    \n",
    "    # model.eval()\n",
    "    # total_loss = 0\n",
    "    # test_batches = 25\n",
    "    # true_positives = 0\n",
    "    # false_positives = 0\n",
    "    # true_negatives = 0\n",
    "    # false_negatives = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for idx, (X, im) in enumerate(tqdm(cff_test_dataloader, total=test_batches)):\n",
    "    #         if idx >= test_batches:\n",
    "    #             break\n",
    "    #         inputs = X.to(device, dtype=torch.int32)\n",
    "    #         im = im.to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "    #         out = model(im, inputs[:,:-7]) # Use only output tokens before this truth term\n",
    "\n",
    "    #         # loss = loss_fn(out.permute(0, 2, 1), inputs.long()) / X.shape[0]\n",
    "    #         loss = numeric_mse_loss(out, inputs) / X.shape[0]\n",
    "            \n",
    "    #         total_loss += loss.item() * X.shape[0]\n",
    "    #         torch.cuda.empty_cache()\n",
    "\n",
    "    #         guesses = out.permute(0, 2, 1).argmax(dim=1)\n",
    "    #         truths = inputs\n",
    "    #         true_positives += ((guesses == truths) * (truths != tokenizer[pad_token])).sum()\n",
    "    #         false_positives += ((guesses != truths) * (truths == tokenizer[pad_token])).sum()\n",
    "    #         true_negatives += ((guesses == truths) * (truths == tokenizer[pad_token])).sum()\n",
    "    #         false_negatives += ((guesses != truths) * (truths != tokenizer[pad_token])).sum()\n",
    "        \n",
    "    #     test_loss = total_loss / (min(test_batches, idx+1)*args['batch_size'])\n",
    "    #     acc, pre, rec, f1 = PerformanceMetrics.all_metrics(\n",
    "    #         tp=true_positives,\n",
    "    #         fp=false_positives,\n",
    "    #         tn=true_negatives,\n",
    "    #         fn=false_negatives\n",
    "    #     )\n",
    "\n",
    "    #     print(f\"Epoch {epoch+1}/{args['epochs']} completed. Train Loss = {train_loss_list[-1]};  Test Loss: {test_loss_list[-1]}\")\n",
    "\n",
    "    #     if args['use_wandb']:\n",
    "    #         wandb.log({\n",
    "    #             \"posttrain_loss\": train_loss,\n",
    "    #             \"posttrain_test_loss\": test_loss,\n",
    "    #             # \"test_accuracy\": acc,\n",
    "    #             # \"test_precision\": pre,\n",
    "    #             # \"test_recall\": rec,\n",
    "    #             # \"test_f1\": f1,\n",
    "    #             \"lr\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "    #         })\n",
    "\n",
    "    # if (epoch+1) % 100 == 0 or epoch+1 == args['epochs']:\n",
    "    # if max_len > 100:\n",
    "    #     torch.save(model, f'models/transformer-{dataset_name}-{epoch+1}.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
