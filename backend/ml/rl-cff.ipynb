{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "\n",
    "from config import device, operators, DecodeType, DecodeInstruction, SamplingType\n",
    "from ml.tokenizer import Tokenizer\n",
    "from ml.fontmodel import DecodeInstruction, FontModel\n",
    "from ml.performance import PerformanceMetrics\n",
    "from parsing.glyph_viz import Visualizer\n",
    "from parsing.tablelist_utils import numbers_first, make_non_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Executing train-cff.ipynb on {device}...\\n-----------------------------\")\n",
    "\n",
    "args = {\n",
    "    \"load_model\": True,\n",
    "    \"train_transformer\": True,\n",
    "    \"min_number\": -500,\n",
    "    \"max_number\": 500,\n",
    "    \"max_seq_len\": 5040,\n",
    "    \"num_layers\": 12,\n",
    "    \"embedding_dim\": 1024,\n",
    "    \"num_heads\": 16,\n",
    "    \"ff_dim\": 4096,\n",
    "    \"use_wandb\": True,\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 1,\n",
    "    \"batch_accumulate\": 4,\n",
    "    \"lr\": 6e-4,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"weight_decay\": 1e-1,\n",
    "    \"gradient_clip\": True,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"label_smoothing\": 0.001,\n",
    "    \"sample_every\": 1,\n",
    "    \"use_scheduler\": True,\n",
    "    \"scheduler_warmup_steps\": 2000,\n",
    "    \"data_type\": torch.bfloat16,\n",
    "    \"vae_beta\": 1e-1,\n",
    "    \"vae_epochs\": 10,\n",
    "    \"vae_lr\": 1e-2,\n",
    "    \"vae_weight_decay\": 1e-5,\n",
    "    \"freeze_embeddings\": False,\n",
    "    \"use_pretrained_embeddings\": False,\n",
    "    \"pretrain_embeddings\": False,\n",
    "    \"pretrain_epochs\": 1,\n",
    "    \"pretrain_batch_size\": 128,\n",
    "    \"pretrain_lr\": 4e-3,\n",
    "    \"pretrain_use_scheduler\": True,\n",
    "    \"pretrain_scheduler_warmup_steps\": 3000,\n",
    "    \"use_pretrained_vit_encoder\": False,\n",
    "    \"pretrain_vit_encoder\": False,\n",
    "    \"pretrain_vit_encoder_epochs\": 1,\n",
    "    \"pretrain_vit_encoder_batch_size\": 128,\n",
    "    \"pretrain_vit_encoder_batch_accumulate\": 1,\n",
    "    \"pretrain_vit_encoder_lr\": 1e-3,\n",
    "    \"pretrain_vit_encoder_weight_decay\": 1e-3,\n",
    "    \"pretrain_vit_encoder_use_scheduler\": True,\n",
    "    \"pretrain_vit_encoder_scheduler_warmup_steps\": 1500,\n",
    "    \"post_train\": False,\n",
    "    \"post_train_epochs\": 1,\n",
    "    \"post_train_batch_size\": 32,\n",
    "    \"post_train_lr\": 6e-4,\n",
    "    \"post_train_kl_penalty\": 0.05,\n",
    "    \"post_train_use_scheduler\": True,\n",
    "    \"post_train_scheduler_warmup_steps\": 2000,\n",
    "}\n",
    "\n",
    "print(\"Posttraining hyperparameters:\")\n",
    "pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = \"<PAD>\"\n",
    "sos_token = \"<SOS>\"\n",
    "eos_token = \"<EOS>\"\n",
    "tokenizer = Tokenizer(\n",
    "    min_number=args['min_number'],\n",
    "    max_number=args['max_number'],\n",
    "    possible_operators=operators,\n",
    "    pad_token=pad_token,\n",
    "    sos_token=sos_token,\n",
    "    eos_token=eos_token\n",
    ")\n",
    "cumulative = True\n",
    "vocab_size = tokenizer.num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_instr = DecodeInstruction( # NOTE: doesn't matter unless loading from .config.txt fails\n",
    "    DecodeType.ANCESTRAL,\n",
    "    SamplingType.MULTINOMIAL,\n",
    "    max_seq_len=args['max_seq_len'],\n",
    "    k=5,\n",
    "    p=0,\n",
    "    temp=0,\n",
    "    beam_size=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_folder = f'~/models'\n",
    "if args['load_model']:\n",
    "    model_pre = torch.load(f'{models_folder}/transformer-basic-33928allchars_centered_scaled_sorted_filtered_cumulative_padded-14.pkl', map_location=device, weights_only=False).to(device)\n",
    "else:\n",
    "    model_pre = FontModel(\n",
    "        num_enc_layers=args['num_layers'],\n",
    "        num_dec_layers=args['num_layers'],\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=args['embedding_dim'],\n",
    "        num_heads=args['num_heads'],\n",
    "        ff_dim=args['ff_dim'],\n",
    "        dropout_rate=args['dropout_rate'],\n",
    "        max_seq_len=args['max_seq_len'],\n",
    "        device=device\n",
    "    ).to(device, dtype=args['data_type'])\n",
    "model = torch.compile(model_pre)\n",
    "original_model = deepcopy(model)\n",
    "original_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (tentative):\n",
    "# FontModel: embedder (DON'T APPLY WEIGHT DECAY)\n",
    "# TransformerDecoder: transformer_decoder_layers (DON'T APPLY WEIGHT DECAY TO RMSNORM), command_encoder, command_decoder, norm_final (DON'T APPLY WEIGHT DECAY)\n",
    "# TransformerEncoder: transformer_encoder_layers (DON'T APPLY WEIGHT DECAY TO RMSNORM), embedder (custom),pos_embed, norm_final (DON'T APPLY WEIGHT DECAY)\n",
    "\n",
    "# We don't want to apply weight decay to layer norms and embeddings\n",
    "no_weight_decay_params = [x for x in model.decoder.embedder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.inverse_embedder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.decoder.transformer_decoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.decoder.norm_final.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.encoder.norm_final.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_encoder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_decoder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_decoder_2a.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_decoder_2b.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.command_decoder_1.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.command_decoder_2.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.W_cn.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.W_cnb.parameters() if x.requires_grad]\n",
    "\n",
    "weight_decay_params = [x for name, x in model.decoder.transformer_decoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for x in model.encoder.pos_embed.parameters() if x.requires_grad]\n",
    "\n",
    "vit_encoder_params_nwd = [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad]# and ('norm' in name or 'bias' in name)]\n",
    "# vit_encoder_params_nwd += [x for name, x in model.encoder.pretrain_reverse_ae.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "vit_encoder_params_nwd += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "vit_encoder_params_nwd += [x for x in model.encoder.norm_final.parameters() if x.requires_grad]\n",
    "# vit_encoder_params_wd = [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "vit_encoder_params_nwd += [x for name, x in model.encoder.pretrain_reverse_ae.named_parameters() if x.requires_grad]# and 'norm' not in name and 'bias' not in name]\n",
    "vit_encoder_params_wd = [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "vit_encoder_params_wd += [x for x in model.encoder.pos_embed.parameters() if x.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "       {'params': weight_decay_params, 'weight_decay': args['weight_decay']},\n",
    "       {'params': no_weight_decay_params, 'weight_decay': args['weight_decay']}\n",
    "    ],\n",
    "    betas=(0.9, 0.95),\n",
    "    lr=args['lr'] \n",
    ")\n",
    "\n",
    "max_len = 33928\n",
    "num_glyphs = 26\n",
    "step_every = 1\n",
    "\n",
    "if args['use_scheduler']:\n",
    "    # scheduler = TransformerScheduler(\n",
    "    #     optimizer=optimizer,\n",
    "    #     dim_embed=args['embedding_dim'],\n",
    "    #     warmup_steps=args['scheduler_warmup_steps']\n",
    "    # )\n",
    "    batches_per_epoch = int(max_len * (num_glyphs // step_every) / args['batch_size'] + 0.5)\n",
    "    scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args['epochs'] * (batches_per_epoch // args['batch_accumulate']), eta_min=1e-5)\n",
    "    scheduler2 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.001, end_factor=1.0, total_iters=args['scheduler_warmup_steps'])\n",
    "    scheduler = torch.optim.lr_scheduler.ChainedScheduler([scheduler1, scheduler2], optimizer=optimizer)\n",
    "\n",
    "dataset_name = f\"~/basic-33928allchars_centered_scaled_sorted_filtered{'_cumulative' if cumulative else ''}_padded\"\n",
    "train_start, train_end = 0, int(0.95 * max_len) * num_glyphs\n",
    "test_start, test_end = train_end, max_len * num_glyphs\n",
    "# max_len = 5\n",
    "# train_start, train_end = 0, 26*max_len\n",
    "# test_start, test_end = 0, 26*max_len\n",
    "cff_dataset = torch.load(f'./{dataset_name}.pt', mmap=True)[train_start:train_end:step_every]\n",
    "cff_dataset_test = torch.load(f'./{dataset_name}.pt', mmap=True)[test_start:test_end:step_every]\n",
    "im_dataset_name = \"~/basic-33928allchars_centered_scaled_sorted_filtered_(128, 128)\"\n",
    "im_dataset = torch.load(f'./{im_dataset_name}.pt', mmap=True)[train_start:train_end:step_every]\n",
    "im_dataset_test = torch.load(f'./{im_dataset_name}.pt', mmap=True)[test_start:test_end:step_every]\n",
    "cff_train_tensor_dataset = TensorDataset(cff_dataset, im_dataset)\n",
    "cff_train_dataloader = DataLoader(cff_train_tensor_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "cff_pretrain_dataloader = DataLoader(cff_train_tensor_dataset, batch_size=args['pretrain_batch_size'], shuffle=True)\n",
    "cff_pretrain_vit_encoder_dataloader = DataLoader(cff_train_tensor_dataset, batch_size=args['pretrain_vit_encoder_batch_size'], shuffle=True)\n",
    "cff_posttrain_dataloader = DataLoader(cff_train_tensor_dataset, batch_size=args['post_train_batch_size'], shuffle=True)\n",
    "cff_test_tensor_dataset = TensorDataset(cff_dataset_test, im_dataset_test)\n",
    "cff_test_dataloader = DataLoader(cff_test_tensor_dataset, batch_size=args['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPost-training model...\\n\")\n",
    "\n",
    "kl_loss = torch.nn.functional.kl_div\n",
    "\n",
    "@torch.no_grad()\n",
    "def value_fn(image_gt, output_tokens):\n",
    "    '''\n",
    "    image_gt: the ground truth image\n",
    "    output_tokens: the model's predicted output\n",
    "    '''\n",
    "    sequence = output_tokens.cpu().detach().numpy().flatten()\n",
    "    torch.cuda.empty_cache()\n",
    "    toks = [tokenizer.reverse_map(tk.item(), use_int=True) for tk in sequence]\n",
    "    toks = [tok for tok in toks if tok != '<PAD2>' and tok != '<PAD>']\n",
    "    if cumulative:\n",
    "        toks = numbers_first(make_non_cumulative(toks, tokenizer), tokenizer, return_string=False)\n",
    "    else:\n",
    "        toks = numbers_first(toks, tokenizer, return_string=False)\n",
    "    viz = Visualizer(output_tokens)\n",
    "    im_pixel_size = (128, 128)\n",
    "    crop_factor = 1\n",
    "    dpi = 1\n",
    "    boundaries = (int((im_pixel_size[0] * (crop_factor * 100 / dpi - 1)) // 2), int((im_pixel_size[1] * (crop_factor * 100 / dpi - 1)) // 2))\n",
    "    im_size_inches = ((im_pixel_size[0] * crop_factor) / dpi, (im_pixel_size[1] * crop_factor) / dpi)\n",
    "    output_image = viz.draw(\n",
    "        display=False,\n",
    "        filename=None,\n",
    "        return_image=True,\n",
    "        center=False,\n",
    "        im_size_inches=im_size_inches,\n",
    "        bounds=(-300, 300),\n",
    "        dpi=dpi\n",
    "    )[None,:,:,0] / 255.0\n",
    "    value = -kl_loss((image_gt + 1) / 2, output_image)\n",
    "    return value\n",
    "\n",
    "@torch.no_grad()\n",
    "def advantage_fn(image_gt, output_tokens):\n",
    "    '''\n",
    "    The value of the current output tokens (\"next state\") minus the value of the previous output tokens (\"current state\")\n",
    "    \n",
    "    image_gt: the ground truth image\n",
    "    output_tokens: the model's predicted output\n",
    "    '''\n",
    "    return value_fn(image_gt, output_tokens) - value_fn(image_gt, output_tokens[:,:-7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.zeros((args['post_train_batch_size'], 0)).to(device)\n",
    "for epoch in range(args['post_train_epochs']):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    last_loss = 0\n",
    "    train_batches = (max_len*(num_glyphs // step_every) // args['post_train_batch_size']) + 1\n",
    "    for idx, (X, im) in enumerate(tqdm(cff_posttrain_dataloader, total=train_batches)):\n",
    "        if idx >= train_batches:\n",
    "            break\n",
    "        inputs = X.to(device, dtype=torch.int32)\n",
    "        im = im.to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "\n",
    "        # output tokens from the current model\n",
    "        out_tokens = model.decode(im, None, decode_instr)[0].cpu().detach().numpy().flatten()\n",
    "        \n",
    "        # token distributions\n",
    "        in_tokens = out_tokens[:,:-7] # note: SOS token is prepended in forward()\n",
    "        dist_new = model(im, in_tokens)\n",
    "        with torch.no_grad():\n",
    "            dist_original = original_model(im, in_tokens)\n",
    "        \n",
    "        adv = torch.Tensor([advantage_fn(im, out_tokens[:,:i*7]) for i in range(1, out_tokens.shape[1]//7+1)]).repeat_interleave(7, dim=1) # (batch_size=1, seq_len=5040)\n",
    "        prob_new = torch.gather(dist_new, dim=-1, index=out_tokens.unsqueeze(1)).squeeze(1)\n",
    "        prob_original = torch.gather(dist_original, dim=-1, index=out_tokens.unsqueeze(1)).squeeze(1)\n",
    "        rel_prob = prob_new / prob_original # (batch_size=1, seq_len)\n",
    "        eps = 0.2\n",
    "        loss = torch.minimum(rel_prob * adv, torch.clip(rel_prob, 1-eps, 1+eps) * adv)\n",
    "\n",
    "        total_loss += loss.item() * X.shape[0]\n",
    "        loss.backward()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if (idx+1) % 1 == 0 or idx == train_batches-1:\n",
    "            if args['gradient_clip']:\n",
    "                torch.nn.utils.clip_grad_value_(model.parameters(), args['gradient_clip_val'])\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if args['post_train_use_scheduler']:\n",
    "                scheduler.step()\n",
    "            diff = total_loss - last_loss\n",
    "            last_loss = total_loss\n",
    "\n",
    "        if args['use_wandb']:\n",
    "            if (idx+1) % 100 == 0 or (idx == train_batches-1 and (epoch+1) % args['sample_every'] == 0):\n",
    "                goal_image, img_arr = decode(epoch, idx)\n",
    "                wandb.log({\n",
    "                    \"posttrain_goal_image\": goal_image,\n",
    "                    \"posttrain_images\": img_arr,\n",
    "                    \"posttrain_loss_step\": diff / (args['batch_accumulate'] * args['post_train_batch_size']),\n",
    "                    \"posttrain_lr_step\": args['post_train_lr'] if not args['post_train_use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                })\n",
    "            elif (idx+1) % 1 == 0:\n",
    "                wandb.log({\n",
    "                    \"posttrain_loss_step\": diff / (args['batch_accumulate'] * args['post_train_batch_size']),\n",
    "                    \"posttrain_lr_step\": args['post_train_lr'] if not args['post_train_use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                })\n",
    "    train_loss = total_loss / (min(train_batches, idx+1)*args['batch_size'])\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    test_batches = 25\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (X, im) in enumerate(tqdm(cff_test_dataloader, total=test_batches)):\n",
    "            if idx >= test_batches:\n",
    "                break\n",
    "            inputs = X.to(device, dtype=torch.int32)\n",
    "            im = im.to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "            out = model(im, inputs[:,:-7]) # Use only output tokens before this truth term\n",
    "\n",
    "            # loss = loss_fn(out.permute(0, 2, 1), inputs.long()) / X.shape[0]\n",
    "            loss = numeric_mse_loss(out, inputs) / X.shape[0]\n",
    "            \n",
    "            total_loss += loss.item() * X.shape[0]\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            guesses = out.permute(0, 2, 1).argmax(dim=1)\n",
    "            truths = inputs\n",
    "            true_positives += ((guesses == truths) * (truths != tokenizer[pad_token])).sum()\n",
    "            false_positives += ((guesses != truths) * (truths == tokenizer[pad_token])).sum()\n",
    "            true_negatives += ((guesses == truths) * (truths == tokenizer[pad_token])).sum()\n",
    "            false_negatives += ((guesses != truths) * (truths != tokenizer[pad_token])).sum()\n",
    "        \n",
    "        test_loss = total_loss / (min(test_batches, idx+1)*args['batch_size'])\n",
    "        acc, pre, rec, f1 = PerformanceMetrics.all_metrics(\n",
    "            tp=true_positives,\n",
    "            fp=false_positives,\n",
    "            tn=true_negatives,\n",
    "            fn=false_negatives\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{args['epochs']} completed. Train Loss = {train_loss_list[-1]};  Test Loss: {test_loss_list[-1]}\")\n",
    "\n",
    "        if args['use_wandb']:\n",
    "            wandb.log({\n",
    "                \"posttrain_loss\": train_loss,\n",
    "                \"posttrain_test_loss\": test_loss,\n",
    "                # \"test_accuracy\": acc,\n",
    "                # \"test_precision\": pre,\n",
    "                # \"test_recall\": rec,\n",
    "                # \"test_f1\": f1,\n",
    "                \"lr\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "            })\n",
    "\n",
    "    # if (epoch+1) % 100 == 0 or epoch+1 == args['epochs']:\n",
    "    # if max_len > 100:\n",
    "    #     torch.save(model, f'models/transformer-{dataset_name}-{epoch+1}.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
