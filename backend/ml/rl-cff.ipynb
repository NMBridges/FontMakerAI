{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ec2-user/FontMakerAI/backend')\n",
    "\n",
    "from config import device, operators, DecodeType, DecodeInstruction, SamplingType\n",
    "from ml.tokenizer import Tokenizer\n",
    "from ml.fontmodel import DecodeInstruction, FontModel\n",
    "from ml.performance import PerformanceMetrics\n",
    "from parsing.glyph_viz import Visualizer\n",
    "from parsing.tablelist_utils import numbers_first, make_non_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Executing train-cff.ipynb on {device}...\\n-----------------------------\")\n",
    "\n",
    "args = {\n",
    "    \"load_model\": True,\n",
    "    \"train_transformer\": True,\n",
    "    \"min_number\": -500,\n",
    "    \"max_number\": 500,\n",
    "    \"max_seq_len\": 5040,\n",
    "    \"num_layers\": 12,\n",
    "    \"embedding_dim\": 1024,\n",
    "    \"num_heads\": 16,\n",
    "    \"ff_dim\": 4096,\n",
    "    \"use_wandb\": True,\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 4,\n",
    "    \"batch_accumulate\": 32,\n",
    "    \"lr\": 1e-4,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"weight_decay\": 1e-1,\n",
    "    \"gradient_clip\": True,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"label_smoothing\": 0.001,\n",
    "    \"sample_every\": 1,\n",
    "    \"use_scheduler\": False,\n",
    "    \"scheduler_warmup_steps\": 2000,\n",
    "    \"data_type\": torch.bfloat16,\n",
    "    \"vae_beta\": 1e-1,\n",
    "    \"vae_epochs\": 10,\n",
    "    \"vae_lr\": 1e-2,\n",
    "    \"vae_weight_decay\": 1e-5,\n",
    "    \"freeze_embeddings\": False,\n",
    "    \"use_pretrained_embeddings\": False,\n",
    "    \"pretrain_embeddings\": False,\n",
    "    \"pretrain_epochs\": 1,\n",
    "    \"pretrain_batch_size\": 128,\n",
    "    \"pretrain_lr\": 4e-3,\n",
    "    \"pretrain_use_scheduler\": True,\n",
    "    \"pretrain_scheduler_warmup_steps\": 3000,\n",
    "    \"use_pretrained_vit_encoder\": False,\n",
    "    \"method\": \"GRPO\",\n",
    "    \"grpo_groups\": 8,\n",
    "    \"grpo_kl_coeff\": 0.04,\n",
    "}\n",
    "\n",
    "print(\"Posttraining hyperparameters:\")\n",
    "pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = \"<PAD>\"\n",
    "sos_token = \"<SOS>\"\n",
    "eos_token = \"<EOS>\"\n",
    "tokenizer = Tokenizer(\n",
    "    min_number=args['min_number'],\n",
    "    max_number=args['max_number'],\n",
    "    possible_operators=operators,\n",
    "    pad_token=pad_token,\n",
    "    sos_token=sos_token,\n",
    "    eos_token=eos_token\n",
    ")\n",
    "cumulative = True\n",
    "vocab_size = tokenizer.num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_instr = DecodeInstruction( # NOTE: doesn't matter unless loading from .config.txt fails\n",
    "    DecodeType.ANCESTRAL,\n",
    "    SamplingType.TEMPERATURE,\n",
    "    max_seq_len=args['max_seq_len'],\n",
    "    k=3,\n",
    "    p=0,\n",
    "    temp=1.0,\n",
    "    beam_size=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_folder = f'../../../models'\n",
    "if args['load_model']:\n",
    "    model_pre = torch.load(f'{models_folder}/transformer-basic-33928allchars_centered_scaled_sorted_filtered_cumulative_padded-14.pkl', map_location=device, weights_only=False).to(device)\n",
    "else:\n",
    "    model_pre = FontModel(\n",
    "        num_enc_layers=args['num_layers'],\n",
    "        num_dec_layers=args['num_layers'],\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=args['embedding_dim'],\n",
    "        num_heads=args['num_heads'],\n",
    "        ff_dim=args['ff_dim'],\n",
    "        dropout_rate=args['dropout_rate'],\n",
    "        max_seq_len=args['max_seq_len'],\n",
    "        device=device\n",
    "    ).to(device, dtype=args['data_type'])\n",
    "# model = torch.compile(model_pre)\n",
    "model = deepcopy(model_pre)\n",
    "original_model = deepcopy(model)\n",
    "original_model.eval()\n",
    "\n",
    "del model_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (tentative):\n",
    "# FontModel: embedder (DON'T APPLY WEIGHT DECAY)\n",
    "# TransformerDecoder: transformer_decoder_layers (DON'T APPLY WEIGHT DECAY TO RMSNORM), command_encoder, command_decoder, norm_final (DON'T APPLY WEIGHT DECAY)\n",
    "# TransformerEncoder: transformer_encoder_layers (DON'T APPLY WEIGHT DECAY TO RMSNORM), embedder (custom),pos_embed, norm_final (DON'T APPLY WEIGHT DECAY)\n",
    "\n",
    "# We don't want to apply weight decay to layer norms and embeddings\n",
    "no_weight_decay_params = [x for x in model.decoder.embedder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.inverse_embedder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.decoder.transformer_decoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.decoder.norm_final.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.encoder.norm_final.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_encoder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_decoder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_decoder_2a.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_decoder_2b.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.command_decoder_1.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.command_decoder_2.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.W_cn.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.W_cnb.parameters() if x.requires_grad]\n",
    "\n",
    "weight_decay_params = [x for name, x in model.decoder.transformer_decoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for x in model.encoder.pos_embed.parameters() if x.requires_grad]\n",
    "\n",
    "vit_encoder_params_nwd = [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad]# and ('norm' in name or 'bias' in name)]\n",
    "# vit_encoder_params_nwd += [x for name, x in model.encoder.pretrain_reverse_ae.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "vit_encoder_params_nwd += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "vit_encoder_params_nwd += [x for x in model.encoder.norm_final.parameters() if x.requires_grad]\n",
    "# vit_encoder_params_wd = [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "vit_encoder_params_nwd += [x for name, x in model.encoder.pretrain_reverse_ae.named_parameters() if x.requires_grad]# and 'norm' not in name and 'bias' not in name]\n",
    "vit_encoder_params_wd = [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "vit_encoder_params_wd += [x for x in model.encoder.pos_embed.parameters() if x.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "       {'params': weight_decay_params, 'weight_decay': args['weight_decay']},\n",
    "       {'params': no_weight_decay_params, 'weight_decay': args['weight_decay']}\n",
    "    ],\n",
    "    betas=(0.9, 0.95),\n",
    "    lr=args['lr'] \n",
    ")\n",
    "\n",
    "max_len = 33928\n",
    "num_glyphs = 26\n",
    "step_every = 1\n",
    "\n",
    "if args['use_scheduler']:\n",
    "    # scheduler = TransformerScheduler(\n",
    "    #     optimizer=optimizer,\n",
    "    #     dim_embed=args['embedding_dim'],\n",
    "    #     warmup_steps=args['scheduler_warmup_steps']\n",
    "    # )\n",
    "    batches_per_epoch = int(max_len * (num_glyphs // step_every) / args['batch_size'] + 0.5)\n",
    "    scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args['epochs'] * (batches_per_epoch // args['batch_accumulate']), eta_min=1e-5)\n",
    "    scheduler2 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.001, end_factor=1.0, total_iters=args['scheduler_warmup_steps'])\n",
    "    scheduler = torch.optim.lr_scheduler.ChainedScheduler([scheduler1, scheduler2], optimizer=optimizer)\n",
    "\n",
    "dataset_dir = \"../../..\"\n",
    "dataset_name = f\"{dataset_dir}/basic-33928allchars_centered_scaled_sorted_filtered{'_cumulative' if cumulative else ''}_padded\"\n",
    "train_start, train_end = 0, int(0.95 * max_len) * num_glyphs\n",
    "test_start, test_end = train_end, max_len * num_glyphs\n",
    "# max_len = 5\n",
    "# train_start, train_end = 0, 26*max_len\n",
    "# test_start, test_end = 0, 26*max_len\n",
    "cff_dataset = torch.load(f'./{dataset_name}.pt', mmap=True)[train_start:train_end:step_every]\n",
    "cff_dataset_test = torch.load(f'./{dataset_name}.pt', mmap=True)[test_start:test_end:step_every]\n",
    "im_dataset_name = f\"{dataset_dir}/basic-33928allchars_centered_scaled_sorted_filtered_(128, 128)\"\n",
    "im_dataset = torch.load(f'./{im_dataset_name}.pt', mmap=True)[train_start:train_end:step_every]\n",
    "im_dataset_test = torch.load(f'./{im_dataset_name}.pt', mmap=True)[test_start:test_end:step_every]\n",
    "cff_train_tensor_dataset = TensorDataset(cff_dataset, im_dataset)\n",
    "cff_train_dataloader = DataLoader(cff_train_tensor_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "cff_test_tensor_dataset = TensorDataset(cff_dataset_test, im_dataset_test)\n",
    "cff_test_dataloader = DataLoader(cff_test_tensor_dataset, batch_size=args['batch_size'] * 4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPost-training model...\\n\")\n",
    "\n",
    "kl_loss = torch.nn.functional.kl_div\n",
    "\n",
    "@torch.no_grad()\n",
    "def value_fns(image_gt, output_tokens, method=\"PPO\"):\n",
    "    '''\n",
    "    image_gt: the ground truth image\n",
    "    output_tokens: the model's predicted output\n",
    "    '''\n",
    "    try:\n",
    "        sequence = output_tokens.cpu().detach().numpy().flatten()\n",
    "        toks = [tokenizer.reverse_map(tk.item(), use_int=True) for tk in sequence]\n",
    "        toks = [tok for tok in toks if tok != '<PAD2>' and tok != '<PAD>']\n",
    "        if cumulative:\n",
    "            toks = numbers_first(make_non_cumulative(toks, tokenizer), tokenizer, return_string=False)\n",
    "        else:\n",
    "            toks = numbers_first(toks, tokenizer, return_string=False)\n",
    "        viz = Visualizer(toks)\n",
    "        im_pixel_size = (128, 128)\n",
    "        crop_factor = 1\n",
    "        dpi = 1\n",
    "        boundaries = (int((im_pixel_size[0] * (crop_factor * 100 / dpi - 1)) // 2), int((im_pixel_size[1] * (crop_factor * 100 / dpi - 1)) // 2))\n",
    "        im_size_inches = ((im_pixel_size[0] * crop_factor) / dpi, (im_pixel_size[1] * crop_factor) / dpi)\n",
    "        if method == \"PPO\":\n",
    "            output_images = viz.rl_draw(\n",
    "                im_size_inches=im_size_inches,\n",
    "                bounds=(-300, 300),\n",
    "                dpi=dpi\n",
    "            ) / 255.0\n",
    "        elif method == \"GRPO\":\n",
    "            output_images = viz.draw(\n",
    "                display=False,\n",
    "                filename=None,\n",
    "                return_image=True,\n",
    "                center=False,\n",
    "                im_size_inches=im_size_inches,\n",
    "                bounds=(-300, 300),\n",
    "                dpi=dpi\n",
    "            )[None,:,:,0] / 255.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error drawing image: {e} for tokens {toks}\")\n",
    "        if method == \"PPO\":\n",
    "            output_images = np.ones((sequence.shape[-1] // 7, 128, 128))\n",
    "        elif method == \"GRPO\":\n",
    "            output_images = np.ones((1, 128, 128))\n",
    "    value = -torch.abs((image_gt + 1) / 2 - torch.Tensor(output_images).to(device))\n",
    "    value = value.mean(dim=(1,2))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(epoch : int, batch_idx : int):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            flag = True\n",
    "            idx = np.random.randint(0, im_dataset_test.shape[0])\n",
    "            im = im_dataset_test[idx:idx+1].to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "            new_decode_instr = deepcopy(decode_instr)\n",
    "            new_decode_instr.sampling_type = SamplingType.GREEDY\n",
    "            sequence = model.decode(im, None, new_decode_instr)[0].cpu().detach().numpy().flatten()\n",
    "            if len(sequence) == decode_instr.max_seq_len:\n",
    "                toks = [tokenizer.reverse_map(tk.item(), use_int=True) for tk in sequence] + ['endchar']\n",
    "            else:\n",
    "                toks = [tokenizer.reverse_map(tk.item(), use_int=True) for tk in sequence[:-1]]\n",
    "\n",
    "            print(\"Before:\", toks)\n",
    "            toks = [tok for tok in toks if tok != '<PAD2>' and tok != '<PAD>']\n",
    "            if cumulative:\n",
    "                toks = numbers_first(make_non_cumulative(toks, tokenizer), tokenizer, return_string=False)\n",
    "                # toks = make_non_cumulative(toks, tokenizer)\n",
    "            else:\n",
    "                toks = numbers_first(toks, tokenizer, return_string=False)\n",
    "                # toks = toks\n",
    "            print(\"After:\", toks)\n",
    "            viz = Visualizer(toks)\n",
    "\n",
    "            if toks[2] != \"rmoveto\" and toks[3] != \"rmoveto\":\n",
    "                raise Exception(\"first operator is not rmoveto\")\n",
    "            \n",
    "            im_pixel_size = (128, 128)\n",
    "            crop_factor = 1\n",
    "            dpi = 1\n",
    "            boundaries = (int((im_pixel_size[0] * (crop_factor * 100 / dpi - 1)) // 2), int((im_pixel_size[1] * (crop_factor * 100 / dpi - 1)) // 2))\n",
    "            im_size_inches = ((im_pixel_size[0] * crop_factor) / dpi, (im_pixel_size[1] * crop_factor) / dpi)\n",
    "            img_arr = viz.draw(\n",
    "                display=False,\n",
    "                filename=None,\n",
    "                return_image=True,\n",
    "                center=False,\n",
    "                im_size_inches=im_size_inches,\n",
    "                bounds=(-300, 300),\n",
    "                dpi=dpi\n",
    "            )[None,:,:,0]\n",
    "            \n",
    "            im_cpu = (im[0] * 127.5 + 127.5).to(device=device, dtype=torch.uint8).cpu().detach().numpy()\n",
    "            img_arr = wandb.Image(np.concatenate([im_cpu, img_arr], axis=2), caption=f\"epoch{epoch+1}_{batch_idx+1}.png\")\n",
    "        except Exception as e:\n",
    "            flag = False\n",
    "            print(f\"Could not generate visualization; generated output was not formatted correctly: {e}\")\n",
    "    model.train()\n",
    "    \n",
    "    if flag:\n",
    "        return (wandb.Image(im[0].to(device=device, dtype=torch.float32).cpu().detach().numpy(), caption=f\"epoch{epoch+1}_{batch_idx+1}.png\"), img_arr)\n",
    "    else:\n",
    "        print(\"Decoding failed.\")\n",
    "        return (None, None)\n",
    "        # return (wandb.Image(im[0].to(device=device, dtype=torch.float32).cpu().detach().numpy(), caption=f\"epoch{epoch+1}_{batch_idx+1}.png\"), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    training = model.training\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(\n",
    "        reduction='sum',\n",
    "        ignore_index=tokenizer[pad_token],\n",
    "        label_smoothing=0.0\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for idx, (X, im) in enumerate(tqdm(cff_test_dataloader)):\n",
    "            inputs = X.to(device, dtype=torch.int32)\n",
    "            im = im.to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "            out = model(im, inputs[:,:-7])\n",
    "\n",
    "            loss = loss_fn(out.permute(0, 2, 1), inputs.long()) / X.shape[0]\n",
    "            # loss = numeric_mse_loss(out, inputs) / X.shape[0]\n",
    "            \n",
    "            total_loss += loss.item() * X.shape[0]\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # guesses = out.permute(0, 2, 1).argmax(dim=1)\n",
    "            # truths = inputs\n",
    "            # true_positives += ((guesses == truths) * (truths != tokenizer[pad_token])).sum()\n",
    "            # false_positives += ((guesses != truths) * (truths == tokenizer[pad_token])).sum()\n",
    "            # true_negatives += ((guesses == truths) * (truths == tokenizer[pad_token])).sum()\n",
    "            # false_negatives += ((guesses != truths) * (truths != tokenizer[pad_token])).sum()\n",
    "        \n",
    "        test_loss = total_loss / ((idx+1)*args['batch_size']*4)\n",
    "        # acc, pre, rec, f1 = PerformanceMetrics.all_metrics(\n",
    "        #     tp=true_positives,\n",
    "        #     fp=false_positives,\n",
    "        #     tn=true_negatives,\n",
    "        #     fn=false_negatives\n",
    "        # )\n",
    "    model.train(training)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['use_wandb']:\n",
    "    wandb.init(\n",
    "        project=\"project-typeface\",\n",
    "        config={\n",
    "            \"model_type\": \"Autoregressive CFF\",\n",
    "            **args\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\n",
    "    'test_loss': test_model()\n",
    "})\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    last_loss = 0\n",
    "    train_batches = (max_len*(num_glyphs // step_every) // args['batch_size']) + 1\n",
    "    for idx, (X, im) in enumerate((cff_train_dataloader)):\n",
    "        if idx >= train_batches:\n",
    "            break\n",
    "        inputs = X.to(device, dtype=torch.int32)\n",
    "        im = im.to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "\n",
    "        if args['method'] == \"GRPO\":\n",
    "            inputs = inputs.repeat_interleave(args[\"grpo_groups\"], 0)\n",
    "            im = im.repeat_interleave(args[\"grpo_groups\"], 0)\n",
    "\n",
    "        # output tokens from the current model\n",
    "        with torch.no_grad():\n",
    "            print(f\"[step {(idx // args['batch_accumulate'])+1}-{(idx % args['batch_accumulate'])+1}/{args['batch_accumulate']}] Decoding {inputs.shape[0]} images...\", end='')\n",
    "            model.eval()\n",
    "            out_tokens = model.rl_decode(im, None, decode_instr)\n",
    "            model.train()\n",
    "            print(\" done.\")\n",
    "        if out_tokens.shape[1] != decode_instr.max_seq_len:\n",
    "            out_tokens = out_tokens[:,:-1]\n",
    "        in_tokens = out_tokens[:,:-7] # note: SOS token is prepended in forward()\n",
    "\n",
    "        # ADVANTAGES\n",
    "        if args['method'] == \"PPO\":\n",
    "            gamma_ = 0.99\n",
    "            lambda_ = 0.95\n",
    "            values = torch.zeros((out_tokens.shape[0], out_tokens.shape[1]//7-1)).to(device, dtype=args['data_type'])\n",
    "            for b in tqdm(range(out_tokens.shape[0])):\n",
    "                index_of_endchar = (out_tokens[b,:] != model.decoder.pad_token[0,0]).nonzero(as_tuple=True)[0][-1].item() + 7\n",
    "                if index_of_endchar == decode_instr.max_seq_len - 1:\n",
    "                    index_of_endchar += 1\n",
    "                values[b,:index_of_endchar//7-2] = value_fns(im[b], out_tokens[b,:index_of_endchar], method=method)[2:]\n",
    "\n",
    "            deltas = torch.Tensor([[gamma_ * values[b][i+1] - values[b][i] for i in range(values.shape[1]-1)] for b in range(out_tokens.shape[0])]).to(device, dtype=args['data_type']) # (batch_size, seq_len=5040)\n",
    "            adv = torch.zeros_like(deltas)\n",
    "            for i in range(adv.shape[1]):\n",
    "                if i == 0:\n",
    "                    adv[:,adv.shape[1]-i-1] = deltas[:,adv.shape[1]-i-1]\n",
    "                else:\n",
    "                    adv[:,adv.shape[1]-i-1] = deltas[:,adv.shape[1]-i-1] + gamma_ * lambda_ * adv[:,adv.shape[1]-i]\n",
    "            adv = adv.repeat_interleave(7, dim=1) # (batch_size, seq_len=5040)\n",
    "        elif args['method'] == \"GRPO\":\n",
    "            rewards = torch.zeros((out_tokens.shape[0],)).to(device, dtype=args['data_type'])\n",
    "            for b in tqdm(range(out_tokens.shape[0])):\n",
    "                index_of_endchar = (out_tokens[b,:] != model.decoder.pad_token[0,0]).nonzero(as_tuple=True)[0][-1].item() + 7\n",
    "                if index_of_endchar == decode_instr.max_seq_len - 1:\n",
    "                    index_of_endchar += 1\n",
    "                rewards[b] = value_fns(im[b], out_tokens[b,:index_of_endchar], method=args['method'])\n",
    "            adv = rewards.unflatten(0, (-1, args[\"grpo_groups\"]))\n",
    "            adv = (adv - adv.mean(dim=1, keepdim=True)) / adv.std(dim=1, keepdim=True)\n",
    "            adv = adv.flatten().unsqueeze(-1).repeat((1, out_tokens.shape[1]))\n",
    "        \n",
    "        if args['method'] == \"PPO\":\n",
    "            k = 1\n",
    "        elif args['method'] == \"GRPO\":\n",
    "            print(f\"[step {(idx // args['batch_accumulate'])+1}-{(idx % args['batch_accumulate'])+1}/{args['batch_accumulate']}] Getting reference distribution...\", end='')\n",
    "            with torch.no_grad():\n",
    "                log_dist_original = original_model(im, in_tokens).log_softmax(dim=-1)\n",
    "            print(\" done.\")\n",
    "            log_prob_original = torch.gather(log_dist_original, dim=-1, index=out_tokens.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            print(f\"[step {(idx // args['batch_accumulate'])+1}-{(idx % args['batch_accumulate'])+1}/{args['batch_accumulate']}] Getting previous distribution...\", end='')\n",
    "            with torch.no_grad():\n",
    "                log_dist_prev = model(im, in_tokens).log_softmax(dim=-1)\n",
    "            print(\" done.\")\n",
    "            log_prob_prev = torch.gather(log_dist_prev, dim=-1, index=out_tokens.unsqueeze(1)).squeeze(1)\n",
    "            k = 1\n",
    "\n",
    "        for _ in range(k):\n",
    "            # token distributions\n",
    "            print(f\"[step {(idx // args['batch_accumulate'])+1}-{(idx % args['batch_accumulate'])+1}/{args['batch_accumulate']}] Getting new distribution...\", end='')\n",
    "            log_dist_new = model(im, in_tokens).log_softmax(dim=-1)\n",
    "            print(\" done.\")\n",
    "            \n",
    "            log_prob_new = torch.gather(log_dist_new, dim=-1, index=out_tokens.unsqueeze(1)).squeeze(1)\n",
    "            rel_prob = (log_prob_new - log_prob_prev).exp() # (batch_size, seq_len)\n",
    "            eps = 0.2\n",
    "            if args['method'] == \"PPO\":\n",
    "                loss = -(torch.minimum(rel_prob[:,14:] * adv, torch.clip(rel_prob[:,14:], 1-eps, 1+eps) * adv) * (out_tokens[:,14:] != tokenizer[pad_token])).mean()\n",
    "            elif args['method'] == \"GRPO\":\n",
    "                kl_approx = ((log_prob_original - log_prob_new).exp() - (log_prob_original - log_prob_new) - 1) * (out_tokens != tokenizer[pad_token])\n",
    "                loss = -(torch.minimum(rel_prob * adv, torch.clip(rel_prob, 1-eps, 1+eps) * adv) * (out_tokens != tokenizer[pad_token])).mean() + args[\"grpo_kl_coeff\"] * kl_approx.mean()\n",
    "\n",
    "            print(f\"[step {(idx // args['batch_accumulate'])+1}-{(idx % args['batch_accumulate'])+1}/{args['batch_accumulate']}] Propagating loss...\", end='')\n",
    "\n",
    "            total_loss += loss.item() * X.shape[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if k > 1 or (idx+1) % args['batch_accumulate'] == 0:\n",
    "                if args['gradient_clip']:\n",
    "                    torch.nn.utils.clip_grad_value_(model.parameters(), args['gradient_clip_val'])\n",
    "                print(\" stepping...\", end='')\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                if args['use_scheduler']:\n",
    "                    scheduler.step()\n",
    "            \n",
    "            print(\" done.\")\n",
    "\n",
    "        diff = total_loss - last_loss\n",
    "        last_loss = total_loss\n",
    "\n",
    "        if args['use_wandb']:\n",
    "            if (idx+1) % (25 * args['batch_accumulate']) == 0:\n",
    "                wandb.log({\n",
    "                    'test_loss': test_model()\n",
    "                })\n",
    "            if (idx+1) % (2 * args['batch_accumulate']) == 0 or (idx == train_batches-1 and (epoch+1) % args['sample_every'] == 0):\n",
    "                goal_image, img_arr = decode(epoch, idx)\n",
    "                wandb.log({\n",
    "                    \"posttrain_goal_image\": goal_image,\n",
    "                    \"posttrain_images\": img_arr,\n",
    "                    \"posttrain_loss_step\": diff / (args['batch_accumulate'] * args['batch_size']),\n",
    "                    \"posttrain_lr_step\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                })\n",
    "            elif (idx+1) % args['batch_accumulate'] == 0:\n",
    "                wandb.log({\n",
    "                    \"posttrain_loss_step\": diff / (args['batch_accumulate'] * args['batch_size']),\n",
    "                    \"posttrain_lr_step\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                })\n",
    "\n",
    "    # if (epoch+1) % 100 == 0 or epoch+1 == args['epochs']:\n",
    "    # if max_len > 100:\n",
    "    #     torch.save(model, f'models/transformer-{dataset_name}-{epoch+1}.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
