{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, dataset\n",
    "import numpy as np\n",
    "from backend.ml.ldm import LDM\n",
    "from backend.ml.ddpm import DDPM\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from backend.config import conv_map, device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Executing train-diffusion.ipynb on {device}...\\n-----------------------------\")\n",
    "\n",
    "args = {\n",
    "    \"load_model\": False,\n",
    "    \"train_vae\": True,\n",
    "    \"train_ddpm\": True,\n",
    "    \"use_wandb\": True,\n",
    "    \"vae_epochs\": 100,\n",
    "    \"ddpm_epochs\": 2500,\n",
    "    \"vae_batch_size\": 26 * 16,\n",
    "    \"ddpm_batch_size\": 26 * 64,\n",
    "    \"vae_lr\": 4e-4,\n",
    "    \"ddpm_lr\": 4e-4,\n",
    "    \"vae_weight_decay\": 1e-5,\n",
    "    \"ddpm_weight_decay\": 1e-5,\n",
    "    \"vae_beta\": 1e-0,\n",
    "    \"gradient_clip\": True,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"T\": 1024,\n",
    "    \"num_glyphs\": 26,\n",
    "    \"label_dim\": 128,\n",
    "    \"loss_fn\": nn.MSELoss(),\n",
    "    \"precision\": torch.float32,\n",
    "    \"rescale_latent\": False,\n",
    "}\n",
    "\n",
    "print(\"Training hyperparameters:\")\n",
    "pprint(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['load_model']:\n",
    "    model = torch.load(f'models/ldm-basic-33928allchars_centered_scaled_sorted_filtered_(128, 128)-10-1-0.pkl', map_location=device).to(device)\n",
    "    if args['train_ddpm']:\n",
    "        model.ddpm = DDPM(diffusion_depth=args['T'], latent_shape=model.enc_dec.latent_shape, label_dim=args['label_dim'], conv_map=conv_map).to(device)\n",
    "else:\n",
    "    model = LDM(diffusion_depth=args['T'], feature_channels=args['num_glyphs'], label_dim=args['label_dim'], conv_map=conv_map).to(device)\n",
    "    # model = DDPM(diffusion_depth=args['T'], latent_shape=(1, 128*6, 128*5), label_dim=args['label_dim'], conv_map=conv_map).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = args['loss_fn']\n",
    "vae_optimizer = torch.optim.AdamW(model.enc_dec.parameters(), lr=args['vae_lr'], weight_decay=args['vae_weight_decay'])\n",
    "ddpm_optimizer = torch.optim.AdamW(model.ddpm.parameters(), lr=args['ddpm_lr'], weight_decay=args['ddpm_weight_decay'])\n",
    "\n",
    "max_len = 33928\n",
    "num_glyphs = 26\n",
    "step_every = 1\n",
    "train_start, train_end = 0, int(0.95 * max_len) * num_glyphs\n",
    "test_start, test_end = train_end, max_len * num_glyphs\n",
    "\n",
    "im_dataset_name = \"basic-33928allchars_centered_scaled_sorted_filtered_(128, 128)\"\n",
    "im_dataset = torch.load(f'./{im_dataset_name}.pt', mmap=True)[train_start:train_end:step_every]\n",
    "im_dataset_test = torch.load(f'./{im_dataset_name}.pt', mmap=True)[test_start:test_end:step_every]\n",
    "vae_train_tensor_dataset = TensorDataset(im_dataset, torch.zeros(im_dataset.shape[0], 1))\n",
    "vae_train_dataloader = DataLoader(vae_train_tensor_dataset, batch_size=args['vae_batch_size'], shuffle=False)\n",
    "vae_test_tensor_dataset = TensorDataset(im_dataset_test, torch.zeros(im_dataset_test.shape[0], 1))\n",
    "vae_test_dataloader = DataLoader(vae_test_tensor_dataset, batch_size=args['vae_batch_size'], shuffle=False)\n",
    "ddpm_train_tensor_dataset = TensorDataset(im_dataset, torch.zeros(im_dataset.shape[0], 1))\n",
    "ddpm_train_dataloader = DataLoader(ddpm_train_tensor_dataset, batch_size=args['ddpm_batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['use_wandb']:\n",
    "    wandb.init(\n",
    "        project=\"project-typeface\",\n",
    "        config={\n",
    "            \"model_type\": \"Diffusion\",\n",
    "            **args\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sampling_traj(ldm, z_T, T, times, y, num_samples):\n",
    "    i = T\n",
    "    prior_eval = ldm.ddpm.training\n",
    "    ldm.ddpm.eval()\n",
    "    x_Ts = [ldm.enc_dec.decode(ldm.denormalize_z(z_T))]\n",
    "    # x_Ts = [z_T]\n",
    "    while i >= 1:\n",
    "        z_T = ldm.ddpm.denoise(z_T, times[i:i+1], y)\n",
    "        i -= 1\n",
    "        if i % (T // (num_samples - 1)) == 0:\n",
    "            x_Ts.append(ldm.enc_dec.decode(ldm.denormalize_z(z_T)))\n",
    "            # x_Ts.append(z_T)\n",
    "    ldm.ddpm.train(prior_eval)\n",
    "    return x_Ts, y\n",
    "\n",
    "def show_img_from_tensor(x_T, scale=True):\n",
    "    new_img = (x_T.cpu().detach().numpy())\n",
    "    if scale:\n",
    "        new_img -= new_img.min()\n",
    "        new_img /= (new_img.max() - new_img.min())\n",
    "    if new_img.shape[0] == 1:\n",
    "        plt.imshow(new_img[0], cmap='gray')\n",
    "        # plt.savefig(\"test.png\")\n",
    "        # plt.show()\n",
    "    else:\n",
    "        plt.imshow(np.moveaxis(new_img, 0, 2))\n",
    "        # plt.savefig(\"test.png\")\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_loss(a, b):\n",
    "    return torch.pow((a - b), 2).sum()\n",
    "\n",
    "def kl_loss(mu, logvar):\n",
    "    return 0.5 * ((torch.pow(mu, 2) + logvar.exp() - logvar - 1)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"train_vae\"]:\n",
    "    model.enc_dec.train()\n",
    "    model.ddpm.eval()\n",
    "    for epoch in range(args['vae_epochs']):\n",
    "        total_loss = 0\n",
    "        test_loss = 0\n",
    "        model.enc_dec.train()\n",
    "        for inp, label in tqdm(vae_train_dataloader):\n",
    "            vae_optimizer.zero_grad()\n",
    "            inp = inp.reshape(inp.shape[0] // args[\"num_glyphs\"], args[\"num_glyphs\"], 128, 128)\n",
    "            inp = inp.to(device, dtype=torch.uint8) / 127.5 - 1.0\n",
    "            inp_hat, mu, logvar = model.enc_dec(inp)\n",
    "            loss = (recon_loss(inp_hat, inp) + args['vae_beta'] * kl_loss(mu, logvar)) / inp.shape[0]\n",
    "            total_loss += loss.item() * inp.shape[0]\n",
    "            loss.backward()\n",
    "            if args['gradient_clip']:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args['gradient_clip_val'])\n",
    "            vae_optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        model.enc_dec.eval()\n",
    "        with torch.no_grad():\n",
    "            for inp, label in tqdm(vae_test_dataloader):\n",
    "                inp = inp.reshape(inp.shape[0] // args[\"num_glyphs\"], args[\"num_glyphs\"], 128, 128)\n",
    "                inp = inp.to(device, dtype=torch.uint8) / 127.5 - 1.0\n",
    "                inp_hat, mu, logvar = model.enc_dec(inp)\n",
    "                loss = (recon_loss(inp_hat, inp) + args['vae_beta'] * kl_loss(mu, logvar)) / inp.shape[0]\n",
    "                test_loss += loss.item() * inp.shape[0]\n",
    "                torch.cuda.empty_cache()\n",
    "        total_loss /= len(vae_train_dataloader.dataset)\n",
    "        test_loss /= len(vae_test_dataloader.dataset)\n",
    "\n",
    "        test_idx = np.random.randint(0, im_dataset_test.shape[0] // args[\"num_glyphs\"])\n",
    "        inp, _ = vae_test_dataloader.dataset[test_idx*args[\"num_glyphs\"]:(test_idx + 1)*args[\"num_glyphs\"]]\n",
    "        inp = inp.reshape(inp.shape[0] // args[\"num_glyphs\"], args[\"num_glyphs\"], 128, 128)\n",
    "        inp = inp.to(device, dtype=torch.uint8) / 127.5 - 1.0\n",
    "        sample, mu, logvar = model.enc_dec(inp)\n",
    "        two_five_five_truth = ((inp[0,:1] + 1.0) * 127.5).clamp(0.0, 255.0).round()\n",
    "        two_five_five_sample = ((sample[0,:1] + 1.0) * 127.5).clamp(0.0, 255.0).round()\n",
    "\n",
    "        wandb.log({\n",
    "            \"vae_train_loss\": total_loss,\n",
    "            \"vae_test_loss\": test_loss,\n",
    "            \"vae_truth\": wandb.Image(two_five_five_truth, caption=f\"epoch{epoch+1}.png\"),\n",
    "            \"vae_recon\": wandb.Image(two_five_five_sample, caption=f\"epoch{epoch+1}.png\"),\n",
    "        })\n",
    "        print(f\"Epoch {epoch+1}/{args['vae_epochs']}: {total_loss=}, {test_loss=}, {mu.abs().mean().item()=}, {logvar.abs().mean().item()=}\")\n",
    "    torch.save(model, f'models/ldm-{im_dataset_name}-{\"\".join(str(args[\"vae_beta\"]).split(\".\"))}-{args[\"vae_epochs\"]}-0.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['rescale_latent']:\n",
    "    print(\"rescaling latent space\")\n",
    "    z = torch.zeros(vae_train_tensor_dataset.tensors[0].shape[0], model.enc_dec.latent_shape[0], model.enc_dec.latent_shape[1], model.enc_dec.latent_shape[2], dtype=args['precision'])\n",
    "    c = 0\n",
    "    for inp, y in tqdm(vae_train_dataloader):\n",
    "        inp = inp.reshape(inp.shape[0] // args[\"num_glyphs\"], args[\"num_glyphs\"], 128, 128)\n",
    "        inp = inp.to(device, dtype=torch.uint8) / 127.5 - 1.0\n",
    "        z[c:c+inp.shape[0]] = (model.enc_dec.encode(inp)).cpu().detach()\n",
    "        c += inp.shape[0]\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    min_z = torch.Tensor([[[z[:,i,j,k].min() for k in range(model.enc_dec.latent_shape[2])] for j in range(model.enc_dec.latent_shape[1])] for i in range(model.enc_dec.latent_shape[0])]).to(device)\n",
    "    max_z = torch.Tensor([[[z[:,i,j,k].max() for k in range(model.enc_dec.latent_shape[2])] for j in range(model.enc_dec.latent_shape[1])] for i in range(model.enc_dec.latent_shape[0])]).to(device)\n",
    "    model.set_latent_range(min_z.min(), max_z.max())\n",
    "    print(min_z.mean(), max_z.mean())\n",
    "else:\n",
    "    model.set_latent_range(-torch.ones(model.enc_dec.latent_shape).to(device), torch.ones(model.enc_dec.latent_shape).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enc_dec.eval()\n",
    "if args['train_ddpm']:\n",
    "    for epoch in range(args['ddpm_epochs']):\n",
    "        total_loss = 0\n",
    "        model.ddpm.train()\n",
    "        for inp, label in tqdm(ddpm_train_dataloader):\n",
    "            ddpm_optimizer.zero_grad()\n",
    "\n",
    "            # # inp (bs * 26, 1, 128, 128)\n",
    "            # inp = inp.reshape(inp.shape[0] // 26, 26, 1, 128, 128)\n",
    "            # r1 = torch.cat(inp[:,0:5].chunk(5, dim=1), dim=-1)\n",
    "            # r2 = torch.cat(inp[:,5:10].chunk(5, dim=1), dim=-1)\n",
    "            # r3 = torch.cat(inp[:,10:15].chunk(5, dim=1), dim=-1)\n",
    "            # r4 = torch.cat(inp[:,15:20].chunk(5, dim=1), dim=-1)\n",
    "            # r5 = torch.cat(inp[:,20:25].chunk(5, dim=1), dim=-1)\n",
    "            # r6 = torch.cat([inp[:,25:26], torch.zeros(inp[:,25:26].shape[0], 1, 128 * 4, 128).to(device)], dim=-1)\n",
    "            # inp = torch.cat((r1, r2, r3, r4, r5, r6), dim=-2)[:,0] # (bs, 1, 128*6, 128*5)\n",
    "\n",
    "            inp = inp.reshape(inp.shape[0] // args[\"num_glyphs\"], args[\"num_glyphs\"], 128, 128)\n",
    "            times_i = torch.randint(1, args['T']+1, (inp.shape[0],)).to(device)\n",
    "            inp = inp.to(device, dtype=torch.uint8) / 127.5 - 1.0\n",
    "            label = label.to(device)\n",
    "            if np.random.random() < 0.1:\n",
    "                label = None\n",
    "            label = None\n",
    "\n",
    "            eps, pred_eps = model(inp, times_i, label)\n",
    "            loss = mse_loss(pred_eps, eps)\n",
    "            total_loss += loss.item() * inp.shape[0]\n",
    "            loss.backward()\n",
    "            ddpm_optimizer.step()\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "        total_loss /= len(ddpm_train_dataloader.dataset)\n",
    "        \n",
    "        if True:\n",
    "            num_images = 9\n",
    "            # plt.show()\n",
    "            shape = (1, 64, 16, 16)\n",
    "            noise = model.enc_dec.reparameterize(torch.zeros(shape).to(device), torch.ones(shape).to(device))[0]\n",
    "            times = torch.IntTensor(np.linspace(0, args['T'], args['T']+1, dtype=int)).to(device)\n",
    "            condition = None#torch.Tensor([[np.random.randint(1)]]).to(device)\n",
    "            traj, cond = sampling_traj(model, noise, args['T'], times, condition, num_images)\n",
    "            for i in range(num_images):\n",
    "                plt.subplot(1, num_images+1, i+1)\n",
    "                show_img_from_tensor(traj[i][0,0:1])\n",
    "            plt.show()\n",
    "            log_images = {\"train_loss\": total_loss}\n",
    "            # for i in range(args['num_glyphs']):\n",
    "            #     two_five_five = ((traj[-1][0,i:i+1] + 1.0) * 127.5).clamp(0.0, 255.0).round()\n",
    "            #     log_images[f\"images_{i}\"] = wandb.Image(two_five_five, caption=f\"epoch{epoch+1}.png\")\n",
    "\n",
    "            base_img = (128, 128)\n",
    "            out_img = torch.ones(1, base_img[0]*6, base_img[1]*5) * 255.0\n",
    "            for i in range(args['num_glyphs']):\n",
    "                r = i // 5\n",
    "                c = i % 5\n",
    "                out_img[:,r*base_img[0]:(r+1)*base_img[0],c*base_img[1]:(c+1)*base_img[1]] = ((traj[-1][0,i:i+1] + 1.0) * 127.5).clamp(0.0, 255.0).round()\n",
    "            log_images[\"all_glyphs\"] = wandb.Image(out_img, caption=f\"epoch{epoch+1}.png\")\n",
    "\n",
    "            if args['use_wandb']:\n",
    "                wandb.log(log_images)\n",
    "\n",
    "        if cond:\n",
    "            print(f\"Condition: {cond.cpu().detach().numpy()[0]}\\n{total_loss=}\\nEpoch {epoch+1}/{args['ddpm_epochs']} finished.\")\n",
    "        else:\n",
    "            print(f\"{total_loss=}\\nEpoch {epoch+1}/{args['ddpm_epochs']} finished.\")\n",
    "\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            torch.save(model, f'models/ldm-{im_dataset_name}-{\"\".join(str(args[\"vae_beta\"]).split(\".\"))}-{args[\"vae_epochs\"]}-{epoch+1}.pkl')\n",
    "\n",
    "    torch.save(model, f'models/ldm-{im_dataset_name}-{\"\".join(str(args[\"vae_beta\"]).split(\".\"))}-{args[\"vae_epochs\"]}-{args[\"ddpm_epochs\"]}.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
