{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './backend/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, dataset\n",
    "from ema_pytorch import EMA\n",
    "import numpy as np\n",
    "from ml.ddpm import DDPM\n",
    "from ml.ldm import LDM\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from config import conv_map, device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Executing train-diffusion.ipynb on {device}...\\n-----------------------------\")\n",
    "\n",
    "models_dir = '../models'\n",
    "base_dir = '..'\n",
    "args = {\n",
    "    \"load_model\": False,\n",
    "    \"model_load_string\": f'{models_dir}/ldm-basic-33928allchars_centered_scaled_sorted_filtered_128_128-0005-100-0.pkl',\n",
    "    \"train_vp\": True,\n",
    "    \"train_ddpm\": True,\n",
    "    \"use_wandb\": True,\n",
    "    \"vp_epochs\": 1000,\n",
    "    \"ddpm_epochs\": 2500,\n",
    "    \"vp_batch_size\": 26 * 16,\n",
    "    \"ddpm_batch_size\": 26 * 64,\n",
    "    \"vp_lr\": 4e-4,\n",
    "    \"ddpm_lr\": 1e-4,\n",
    "    \"vp_weight_decay\": 1e-5,\n",
    "    \"ddpm_weight_decay\": 1e-5,\n",
    "    \"vp_beta\": 5e-3,\n",
    "    \"gradient_clip\": False,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"T\": 25,\n",
    "    \"num_glyphs\": 26,\n",
    "    \"embedding_dim\": 2048,\n",
    "    \"num_layers\": 24,\n",
    "    \"num_heads\": 32,\n",
    "    \"label_dim\": 128,\n",
    "    \"sample_every\": 50,\n",
    "    \"loss_fn\": nn.MSELoss(),\n",
    "    \"precision\": torch.float32,\n",
    "    \"rescale_latent\": False,\n",
    "    \"save_after_scaling\": True,\n",
    "    \"vp_use_scheduler\": True,\n",
    "    \"vp_scheduler_warmup_steps\": 2000,\n",
    "    \"ddpm_use_scheduler\": False,\n",
    "    \"ddpm_scheduler_warmup_steps\": 2000,\n",
    "}\n",
    "\n",
    "print(\"Training hyperparameters:\")\n",
    "pprint(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['load_model']:\n",
    "    model = torch.load(args['model_load_string'], map_location=device, weights_only=False).to(device, dtype=args['precision'])\n",
    "    model.ddpm = DDPM(diffusion_depth=args['T'], num_layers=args['num_layers'], embedding_dim=args['embedding_dim'], num_glyphs=args['num_glyphs'], num_heads=args['num_heads'], cond_dim=args['label_dim']).to(device, dtype=args['precision'])\n",
    "else:\n",
    "    # model = DDPM(diffusion_depth=args['T'], latent_shape=(1, 128*6, 128*5), label_dim=args['label_dim'], conv_map=conv_map).to(device, dtype=args['precision'])\n",
    "    model = LDM(diffusion_depth=args['T'], embedding_dim=args['embedding_dim'], num_glyphs=args['num_glyphs'], label_dim=args['label_dim'], num_layers=args['num_layers'], num_heads=args['num_heads'], cond_dim=args['label_dim']).to(device, dtype=args['precision'])\n",
    "\n",
    "model = torch.compile(model)\n",
    "ema = EMA(\n",
    "    model,\n",
    "    beta=0.9999,\n",
    "    update_after_step=5,\n",
    "    update_every=1\n",
    ").to(dtype=args['precision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = args['loss_fn']\n",
    "vp_params = [param for param in model.enc_dec.parameters() if param.requires_grad]\n",
    "vp_optimizer = torch.optim.AdamW(vp_params, lr=args['vp_lr'], weight_decay=args['vp_weight_decay'])\n",
    "\n",
    "ddpm_params = [param for param in model.ddpm.parameters() if param.requires_grad]\n",
    "ddpm_optimizer = torch.optim.AdamW(ddpm_params, lr=args['ddpm_lr'], weight_decay=args['ddpm_weight_decay'])\n",
    "\n",
    "max_len = 33928\n",
    "num_glyphs = 26\n",
    "step_every = 1\n",
    "train_start, train_end = 0, int(0.95 * max_len) * num_glyphs\n",
    "test_start, test_end = train_end, max_len * num_glyphs\n",
    "\n",
    "if args['vp_use_scheduler']:\n",
    "    vp_batches_per_epoch = int(max_len * (num_glyphs // step_every) / args['vp_batch_size'] + 0.5)\n",
    "    vp_scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(vp_optimizer, T_max=args['vp_epochs'] * vp_batches_per_epoch, eta_min=1e-5)\n",
    "    vp_scheduler2 = torch.optim.lr_scheduler.LinearLR(vp_optimizer, start_factor=0.001, end_factor=1.0, total_iters=args['vp_scheduler_warmup_steps'])\n",
    "    vp_scheduler = torch.optim.lr_scheduler.ChainedScheduler([vp_scheduler1, vp_scheduler2], optimizer=vp_optimizer)\n",
    "\n",
    "if args['ddpm_use_scheduler']:\n",
    "    ddpm_batches_per_epoch = int(max_len * (num_glyphs // step_every) / args['ddpm_batch_size'] + 0.5)\n",
    "    ddpm_scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(ddpm_optimizer, T_max=args['ddpm_epochs'] * ddpm_batches_per_epoch, eta_min=1e-5)\n",
    "    ddpm_scheduler2 = torch.optim.lr_scheduler.LinearLR(ddpm_optimizer, start_factor=0.001, end_factor=1.0, total_iters=args['ddpm_scheduler_warmup_steps'])\n",
    "    ddpm_scheduler = torch.optim.lr_scheduler.ChainedScheduler([ddpm_scheduler1, ddpm_scheduler2], optimizer=ddpm_optimizer)\n",
    "\n",
    "im_dataset_name = f\"basic-33928allchars_centered_scaled_sorted_filtered_(128, 128)\"\n",
    "im_dataset = torch.load(f'{base_dir}/{im_dataset_name}.pt', mmap=True)[train_start:train_end:step_every]\n",
    "im_dataset_test = torch.load(f'{base_dir}/{im_dataset_name}.pt', mmap=True)[test_start:test_end:step_every]\n",
    "vp_train_tensor_dataset = TensorDataset(im_dataset, torch.zeros(im_dataset.shape[0], 1))\n",
    "vp_train_dataloader = DataLoader(vp_train_tensor_dataset, batch_size=args['vp_batch_size'], shuffle=False)\n",
    "vp_test_tensor_dataset = TensorDataset(im_dataset_test, torch.zeros(im_dataset_test.shape[0], 1))\n",
    "vp_test_dataloader = DataLoader(vp_test_tensor_dataset, batch_size=args['vp_batch_size'], shuffle=False)\n",
    "ddpm_train_tensor_dataset = TensorDataset(im_dataset, torch.zeros(im_dataset.shape[0], 1))\n",
    "ddpm_train_dataloader = DataLoader(ddpm_train_tensor_dataset, batch_size=args['ddpm_batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['use_wandb']:\n",
    "    wandb.init(\n",
    "        project=\"project-typeface\",\n",
    "        config={\n",
    "            \"model_type\": \"Diffusion-Transformer\",\n",
    "            **args\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sampling_traj(ldm, z_T, T, times, y, num_samples):\n",
    "    i = T\n",
    "    prior_eval = ldm.training\n",
    "    ldm.eval()\n",
    "    x_Ts = [ldm.latent_to_feature(z_T)]\n",
    "    while i >= 1:\n",
    "        z_T = ldm.denoise(z_T, times[i:i+1], y)\n",
    "        i -= 1\n",
    "        if i % (T // (num_samples - 1)) == 0:\n",
    "            x_Ts.append(ldm.latent_to_feature(z_T))\n",
    "    ldm.train(prior_eval)\n",
    "    return x_Ts, y\n",
    "\n",
    "def show_img_from_tensor(x_T, scale=True):\n",
    "    new_img = (x_T.cpu().detach().numpy())\n",
    "    if scale:\n",
    "        new_img -= new_img.min()\n",
    "        new_img /= (new_img.max() - new_img.min())\n",
    "    if new_img.shape[0] == 1:\n",
    "        plt.imshow(new_img[0], cmap='gray')\n",
    "        # plt.savefig(\"test.png\")\n",
    "        # plt.show()\n",
    "    else:\n",
    "        plt.imshow(np.moveaxis(new_img, 0, 2))\n",
    "        # plt.savefig(\"test.png\")\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_loss(a, b):\n",
    "    return torch.pow((a - b), 2).sum()\n",
    "\n",
    "def kl_loss(mu, logvar):\n",
    "    return 0.5 * ((torch.pow(mu, 2) + logvar.exp() - logvar - 1)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain vector projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"train_vp\"]:\n",
    "    for epoch in range(args['vp_epochs']):\n",
    "        total_loss = 0\n",
    "        test_loss = 0\n",
    "        model.train()\n",
    "        for step, (inp, label) in enumerate(tqdm(vp_train_dataloader)):\n",
    "            vp_optimizer.zero_grad()\n",
    "            inp = inp.reshape(inp.shape[0] // args[\"num_glyphs\"], args[\"num_glyphs\"], 128, 128)\n",
    "            inp = inp.to(device, dtype=torch.uint8) / 127.5 - 1.0\n",
    "            inp_hat, mu, logvar = model.enc_dec(inp)\n",
    "            loss = (recon_loss(inp_hat, inp) + kl_loss(mu, logvar)) / inp.shape[0]\n",
    "            total_loss += loss.item() * inp.shape[0]\n",
    "            loss.backward()\n",
    "            if args['gradient_clip']:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args['gradient_clip_val'])\n",
    "            vp_optimizer.step()\n",
    "            if args['vp_use_scheduler']:\n",
    "                vp_scheduler.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if args['use_wandb']:\n",
    "                if (step+1) % 100 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        model.eval()\n",
    "                        test_idx = np.random.randint(0, im_dataset_test.shape[0] // args[\"num_glyphs\"])\n",
    "                        inp, _ = vp_test_dataloader.dataset[test_idx*args[\"num_glyphs\"]:(test_idx + 1)*args[\"num_glyphs\"]]\n",
    "                        inp = inp.reshape(inp.shape[0] // args[\"num_glyphs\"], args[\"num_glyphs\"], 128, 128)\n",
    "                        inp = inp.to(device, dtype=torch.uint8) / 127.5 - 1.0\n",
    "                        sample, mu, logvar = model.enc_dec(inp)\n",
    "                        two_five_five_truth = ((inp[0,:1] + 1.0) * 127.5).clamp(0.0, 255.0).round()\n",
    "                        two_five_five_sample = ((sample[0,:1] + 1.0) * 127.5).clamp(0.0, 255.0).round()\n",
    "                        model.train()\n",
    "\n",
    "                        wandb.log({\n",
    "                            \"vp_truth\": wandb.Image(two_five_five_truth, caption=f\"epoch{epoch+1}_{step}.png\"),\n",
    "                            \"vp_recon\": wandb.Image(np.concatenate((two_five_five_truth.cpu().detach().numpy(), two_five_five_sample.cpu().detach().numpy()), axis=2), caption=f\"epoch{epoch+1}_{step}.png\"),\n",
    "                            \"mu_abs_mean\": mu.abs().mean().item(),\n",
    "                            \"logvar_abs_mean\": logvar.abs().mean().item(),\n",
    "                            \"vp_train_loss_step\": loss.item(),\n",
    "                            \"vp_lr\": vp_scheduler.get_last_lr()[0] if args['vp_use_scheduler'] else args['vp_lr']\n",
    "                        })\n",
    "                else:\n",
    "                    wandb.log({\n",
    "                        \"vp_train_loss_step\": loss.item(),\n",
    "                        \"mu_abs_mean\": mu.abs().mean().item(),\n",
    "                        \"logvar_abs_mean\": logvar.abs().mean().item(),\n",
    "                        \"vp_lr\": vp_scheduler.get_last_lr()[0] if args['vp_use_scheduler'] else args['vp_lr']\n",
    "                    })\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inp, label in tqdm(vp_test_dataloader):\n",
    "                inp = inp.reshape(inp.shape[0] // args[\"num_glyphs\"], args[\"num_glyphs\"], 128, 128)\n",
    "                inp = inp.to(device, dtype=torch.uint8) / 127.5 - 1.0\n",
    "                inp_hat, mu, logvar = model.enc_dec(inp)\n",
    "                loss = (recon_loss(inp_hat, inp) + kl_loss(mu, logvar)) / inp.shape[0]\n",
    "                test_loss += loss.item() * inp.shape[0]\n",
    "                torch.cuda.empty_cache()\n",
    "        total_loss /= len(vp_train_dataloader.dataset)\n",
    "        test_loss /= len(vp_test_dataloader.dataset)\n",
    "\n",
    "        test_idx = np.random.randint(0, im_dataset_test.shape[0] // args[\"num_glyphs\"])\n",
    "        inp, _ = vp_test_dataloader.dataset[test_idx*args[\"num_glyphs\"]:(test_idx + 1)*args[\"num_glyphs\"]]\n",
    "        inp = inp.reshape(inp.shape[0] // args[\"num_glyphs\"], args[\"num_glyphs\"], 128, 128)\n",
    "        inp = inp.to(device, dtype=torch.uint8) / 127.5 - 1.0\n",
    "        sample, mu, logvar = model.enc_dec(inp)\n",
    "        two_five_five_truth = ((inp[0,:1] + 1.0) * 127.5).clamp(0.0, 255.0).round()\n",
    "        two_five_five_sample = ((sample[0,:1] + 1.0) * 127.5).clamp(0.0, 255.0).round()\n",
    "\n",
    "        wandb.log({\n",
    "            \"vp_test_loss\": test_loss,\n",
    "            \"vp_truth\": wandb.Image(two_five_five_truth, caption=f\"epoch{epoch+1}.png\"),\n",
    "            \"vp_recon\": wandb.Image(np.concatenate((two_five_five_truth.cpu().detach().numpy(), two_five_five_sample.cpu().detach().numpy()), axis=2), caption=f\"epoch{epoch+1}.png\"),\n",
    "        })\n",
    "        print(f\"Epoch {epoch+1}/{args['vp_epochs']}: {total_loss=}, {test_loss=}, {mu.abs().mean().item()=}, {logvar.abs().mean().item()=}\")\n",
    "    torch.save(model.state_dict(), f'{models_dir}/ldm-{im_dataset_name}-{\"\".join(str(args[\"vp_beta\"]).split(\".\"))}-{args[\"vp_epochs\"]}-0.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['rescale_latent']:\n",
    "    print(\"rescaling latent space\")\n",
    "    min_z = torch.zeros(1, model.enc_dec.latent_shape[-2], model.enc_dec.latent_shape[-1], dtype=args['precision'])\n",
    "    max_z = torch.zeros(1, model.enc_dec.latent_shape[-2], model.enc_dec.latent_shape[-1], dtype=args['precision'])\n",
    "    for inp, y in tqdm(vp_train_dataloader):\n",
    "        inp = inp.reshape(inp.shape[0] // args[\"num_glyphs\"], args[\"num_glyphs\"], 128, 128)\n",
    "        inp = inp.to(device, dtype=torch.uint8) / 127.5 - 1.0\n",
    "        z = (model.enc_dec.encode(inp)).cpu().detach()\n",
    "        min_z = torch.minimum(min_z, torch.min(z, dim=0).values)\n",
    "        max_z = torch.maximum(max_z, torch.max(z, dim=0).values)\n",
    "        torch.cuda.empty_cache()\n",
    "    model.enc_dec.z_min[0:] = min_z.to(device)\n",
    "    model.enc_dec.z_max[0:] = max_z.to(device)\n",
    "    print(min_z.mean(), max_z.mean())\n",
    "    if args['save_after_scaling']:\n",
    "        torch.save(model.state_dict(), args['model_load_string'])\n",
    "else:\n",
    "    pass\n",
    "    # model.set_latent_range(-torch.ones(model.enc_dec.latent_shape).to(device), torch.ones(model.enc_dec.latent_shape).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(epoch, step):\n",
    "    num_images = 9\n",
    "    # plt.show()\n",
    "    # shape = (1, 1, 2048)\n",
    "    shape = (1, args['num_glyphs'], 2048)\n",
    "    # x = ddpm_train_dataloader.dataset[0:args['num_glyphs']][0].to(device, dtype=args['precision']).reshape(1, args['num_glyphs'], 128, 128) / 127.5 - 1.0\n",
    "    # noise = model.noise_pred.embedder.vector_projector.encode(x)\n",
    "    # recon = model.noise_pred.embedder.vector_projector.decode(noise)\n",
    "    noise = model.ddpm.reparameterize(torch.zeros(shape).to(device, dtype=args['precision']), torch.ones(shape).to(device, dtype=args['precision']))[0]\n",
    "    times = torch.IntTensor(np.linspace(0, args['T'], args['T']+1, dtype=int)).to(device)\n",
    "    condition = None#torch.Tensor([[np.random.randint(1)]]).to(device)\n",
    "    traj, cond = sampling_traj(model, noise, args['T'], times, condition, num_images)\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images+1, i+1)\n",
    "        # show_img_from_tensor(traj[i][0,0:1])\n",
    "    plt.show()\n",
    "    # for i in range(args['num_glyphs']):\n",
    "    #     two_five_five = ((traj[-1][0,i:i+1] + 1.0) * 127.5).clamp(0.0, 255.0).round()\n",
    "    #     log_images[f\"images_{i}\"] = wandb.Image(two_five_five, caption=f\"epoch{epoch+1}.png\")\n",
    "\n",
    "    base_img = (128, 128)\n",
    "    out_img = torch.ones(1, base_img[0]*6, base_img[1]*5) * 255.0\n",
    "    # for i in range(1):\n",
    "    for i in range(args['num_glyphs']):\n",
    "        r = i // 5\n",
    "        c = i % 5\n",
    "        out_img[:,r*base_img[0]:(r+1)*base_img[0],c*base_img[1]:(c+1)*base_img[1]] = ((traj[-1][0,i:i+1] + 1.0) * 127.5).clamp(0.0, 255.0).round()\n",
    "        # out_img[:,r*base_img[0]:(r+1)*base_img[0],c*base_img[1]:(c+1)*base_img[1]] = ((recon[0,i:i+1] + 1.0) * 127.5).clamp(0.0, 255.0).round()\n",
    "    log_images = {\"all_glyphs\": wandb.Image(out_img, caption=f\"epoch{epoch+1}_{step}.png\")}\n",
    "\n",
    "    return log_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['train_ddpm']:\n",
    "    for epoch in range(args['ddpm_epochs']):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for step, (inp, label) in enumerate(tqdm(ddpm_train_dataloader)):\n",
    "            ddpm_optimizer.zero_grad()\n",
    "\n",
    "            inp = inp.reshape(inp.shape[0] // args[\"num_glyphs\"], args[\"num_glyphs\"], 128, 128)\n",
    "            times_i = torch.randint(1, args['T']+1, (inp.shape[0],)).to(device)\n",
    "            inp = inp.to(device, dtype=args['precision']) / 127.5 - 1.0\n",
    "            label = label.to(device)\n",
    "            if np.random.random() < 0.1:\n",
    "                label = None\n",
    "            label = None\n",
    "            \n",
    "            eps, pred_eps = model(inp, times_i, label)\n",
    "            loss = recon_loss(pred_eps, eps) / inp.shape[0]\n",
    "            total_loss += loss.item() * inp.shape[0]\n",
    "            loss.backward()\n",
    "            ddpm_optimizer.step()\n",
    "            if args['ddpm_use_scheduler']:\n",
    "                ddpm_scheduler.step()\n",
    "            ema.update()\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            log_images = {\n",
    "                \"train_loss_step\": loss.item(),\n",
    "                \"ddpm_lr\": ddpm_scheduler.get_last_lr()[0] if args['ddpm_use_scheduler'] else args['ddpm_lr']\n",
    "            }\n",
    "            if (step+1) % args[\"sample_every\"] == 0:\n",
    "                log_images.update(sample(epoch, step))\n",
    "            if args['use_wandb']:\n",
    "                wandb.log(log_images)\n",
    "        total_loss /= len(ddpm_train_dataloader.dataset)\n",
    "        \n",
    "\n",
    "        print(f\"{total_loss=}\\nEpoch {epoch+1}/{args['ddpm_epochs']} finished.\")\n",
    "\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            torch.save(model.state_dict(), f'{models_dir}/ldm-{im_dataset_name}-{\"\".join(str(args[\"vp_beta\"]).split(\".\"))}-{args[\"vp_epochs\"]}-{epoch+1}.pkl')\n",
    "\n",
    "    torch.save(model.state_dict(), f'{models_dir}/ldm-{im_dataset_name}-{\"\".join(str(args[\"vp_beta\"]).split(\".\"))}-{args[\"vp_epochs\"]}-{args[\"ddpm_epochs\"]}.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
