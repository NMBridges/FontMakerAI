{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from config import conv_map, device, operators\n",
    "\n",
    "from backend.ml.fontmodel import (FontModel, DecodeInstruction, DecodeType, SamplingType, TransformerScheduler)\n",
    "from backend.ml.tokenizer import Tokenizer\n",
    "from backend.parsing.glyph_viz import Visualizer\n",
    "from backend.ml.performance import PerformanceMetrics\n",
    "from backend.parsing.tablelist_utils import numbers_first, make_non_cumulative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Executing train-cff.ipynb on {device}...\\n-----------------------------\")\n",
    "\n",
    "args = {\n",
    "    \"load_model\": True,\n",
    "    \"train_transformer\": True,\n",
    "    \"min_number\": -500,\n",
    "    \"max_number\": 500,\n",
    "    \"max_seq_len\": 5040,\n",
    "    \"num_layers\": 12,\n",
    "    \"embedding_dim\": 1024,\n",
    "    \"num_heads\": 16,\n",
    "    \"ff_dim\": 4096,\n",
    "    \"use_wandb\": True,\n",
    "    \"epochs\": 15,\n",
    "    \"batch_size\": 32,\n",
    "    \"batch_accumulate\": 4,\n",
    "    \"lr\": 6e-4,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"weight_decay\": 1e-1,\n",
    "    \"gradient_clip\": True,\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"label_smoothing\": 0.001,\n",
    "    \"sample_every\": 1,\n",
    "    \"use_scheduler\": True,\n",
    "    \"scheduler_warmup_steps\": 2000,\n",
    "    \"data_type\": torch.bfloat16,\n",
    "    \"vae_beta\": 1e-1,\n",
    "    \"vae_epochs\": 10,\n",
    "    \"vae_lr\": 1e-2,\n",
    "    \"vae_weight_decay\": 1e-5,\n",
    "    \"freeze_embeddings\": False,\n",
    "    \"use_pretrained_embeddings\": False,\n",
    "    \"pretrain_embeddings\": False,\n",
    "    \"pretrain_epochs\": 1,\n",
    "    \"pretrain_batch_size\": 128,\n",
    "    \"pretrain_lr\": 4e-3,\n",
    "    \"pretrain_use_scheduler\": True,\n",
    "    \"pretrain_scheduler_warmup_steps\": 3000,\n",
    "    \"use_pretrained_vit_encoder\": False,\n",
    "    \"pretrain_vit_encoder\": False,\n",
    "    \"pretrain_vit_encoder_epochs\": 1,\n",
    "    \"pretrain_vit_encoder_batch_size\": 128,\n",
    "    \"pretrain_vit_encoder_batch_accumulate\": 1,\n",
    "    \"pretrain_vit_encoder_lr\": 1e-3,\n",
    "    \"pretrain_vit_encoder_weight_decay\": 1e-3,\n",
    "    \"pretrain_vit_encoder_use_scheduler\": True,\n",
    "    \"pretrain_vit_encoder_scheduler_warmup_steps\": 1500,\n",
    "    \"post_train\": False,\n",
    "    \"post_train_epochs\": 1,\n",
    "    \"post_train_batch_size\": 32,\n",
    "    \"post_train_lr\": 6e-4,\n",
    "    \"post_train_kl_penalty\": 0.05,\n",
    "    \"post_train_use_scheduler\": True,\n",
    "    \"post_train_scheduler_warmup_steps\": 2000,\n",
    "}\n",
    "\n",
    "print(\"Training hyperparameters:\")\n",
    "pprint(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = \"<PAD>\"\n",
    "sos_token = \"<SOS>\"\n",
    "eos_token = \"<EOS>\"\n",
    "tokenizer = Tokenizer(\n",
    "    min_number=args['min_number'],\n",
    "    max_number=args['max_number'],\n",
    "    possible_operators=operators,\n",
    "    pad_token=pad_token,\n",
    "    sos_token=sos_token,\n",
    "    eos_token=eos_token\n",
    ")\n",
    "cumulative = True\n",
    "vocab_size = tokenizer.num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_instr = DecodeInstruction( # NOTE: doesn't matter unless loading from .config.txt fails\n",
    "    DecodeType.ANCESTRAL,\n",
    "    SamplingType.GREEDY,\n",
    "    max_seq_len=args['max_seq_len'],\n",
    "    k=5,\n",
    "    p=0,\n",
    "    temp=0,\n",
    "    beam_size=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_folder = f'~/models'\n",
    "if args['load_model']:\n",
    "    model_pre = torch.load(f'{models_folder}/transformer-basic-33928allchars_centered_scaled_sorted_filtered_cumulative_padded-14.pkl', map_location=device, weights_only=False).to(device)\n",
    "else:\n",
    "    model_pre = FontModel(\n",
    "        num_enc_layers=args['num_layers'],\n",
    "        num_dec_layers=args['num_layers'],\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=args['embedding_dim'],\n",
    "        num_heads=args['num_heads'],\n",
    "        ff_dim=args['ff_dim'],\n",
    "        dropout_rate=args['dropout_rate'],\n",
    "        max_seq_len=args['max_seq_len'],\n",
    "        device=device\n",
    "    ).to(device, dtype=args['data_type'])\n",
    "model = torch.compile(model_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(modela):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, modela.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print(f\"Number of trainable parameters: {params}\")\n",
    "    return params\n",
    "\n",
    "x = count_params(model)\n",
    "y = count_params(model.encoder)\n",
    "z = count_params(model.decoder)\n",
    "w = count_params(model.encoder.embedder)\n",
    "v = count_params(model.decoder.embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (tentative):\n",
    "# FontModel: embedder (DON'T APPLY WEIGHT DECAY)\n",
    "# TransformerDecoder: transformer_decoder_layers (DON'T APPLY WEIGHT DECAY TO RMSNORM), command_encoder, command_decoder, norm_final (DON'T APPLY WEIGHT DECAY)\n",
    "# TransformerEncoder: transformer_encoder_layers (DON'T APPLY WEIGHT DECAY TO RMSNORM), embedder (custom),pos_embed, norm_final (DON'T APPLY WEIGHT DECAY)\n",
    "\n",
    "# We don't want to apply weight decay to layer norms and embeddings\n",
    "no_weight_decay_params = [x for x in model.decoder.embedder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.inverse_embedder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.decoder.transformer_decoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.decoder.norm_final.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.encoder.norm_final.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_encoder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_decoder.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_decoder_2a.parameters() if x.requires_grad]\n",
    "no_weight_decay_params += [x for x in model.decoder.command_decoder_2b.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.command_decoder_1.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.command_decoder_2.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.W_cn.parameters() if x.requires_grad]\n",
    "# no_weight_decay_params += [x for x in model.decoder.W_cnb.parameters() if x.requires_grad]\n",
    "\n",
    "weight_decay_params = [x for name, x in model.decoder.transformer_decoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "weight_decay_params += [x for x in model.encoder.pos_embed.parameters() if x.requires_grad]\n",
    "\n",
    "vit_encoder_params_nwd = [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad]# and ('norm' in name or 'bias' in name)]\n",
    "# vit_encoder_params_nwd += [x for name, x in model.encoder.pretrain_reverse_ae.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "vit_encoder_params_nwd += [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and ('norm' in name or 'bias' in name)]\n",
    "vit_encoder_params_nwd += [x for x in model.encoder.norm_final.parameters() if x.requires_grad]\n",
    "# vit_encoder_params_wd = [x for name, x in model.encoder.embedder.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "vit_encoder_params_nwd += [x for name, x in model.encoder.pretrain_reverse_ae.named_parameters() if x.requires_grad]# and 'norm' not in name and 'bias' not in name]\n",
    "vit_encoder_params_wd = [x for name, x in model.encoder.transformer_encoder_layers.named_parameters() if x.requires_grad and 'norm' not in name and 'bias' not in name]\n",
    "vit_encoder_params_wd += [x for x in model.encoder.pos_embed.parameters() if x.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "       {'params': weight_decay_params, 'weight_decay': args['weight_decay']},\n",
    "       {'params': no_weight_decay_params, 'weight_decay': args['weight_decay']}\n",
    "    ],\n",
    "    betas=(0.9, 0.95),\n",
    "    lr=args['lr'] \n",
    ")\n",
    "\n",
    "pretrain_optimizer = torch.optim.AdamW(no_weight_decay_params, weight_decay=0.0, betas=(0.9, 0.95), lr=args['pretrain_lr'])\n",
    "posttrain_optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "       {'params': weight_decay_params, 'weight_decay': 0.0},\n",
    "       {'params': no_weight_decay_params, 'weight_decay': 0.0}\n",
    "    ],\n",
    "    betas=(0.9, 0.95),\n",
    "    lr=args['post_train_lr']\n",
    ")\n",
    "\n",
    "pretrain_vit_encoder_optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {'params': vit_encoder_params_wd, 'weight_decay': args['pretrain_vit_encoder_weight_decay']},\n",
    "        {'params': vit_encoder_params_nwd, 'weight_decay': 0.0},\n",
    "    ],\n",
    "    betas=(0.9, 0.95),\n",
    "    lr=args['pretrain_vit_encoder_lr']\n",
    ")\n",
    "\n",
    "max_len = 33928\n",
    "num_glyphs = 26\n",
    "step_every = 1\n",
    "\n",
    "if args['use_scheduler']:\n",
    "    # scheduler = TransformerScheduler(\n",
    "    #     optimizer=optimizer,\n",
    "    #     dim_embed=args['embedding_dim'],\n",
    "    #     warmup_steps=args['scheduler_warmup_steps']\n",
    "    # )\n",
    "    batches_per_epoch = int(max_len * (num_glyphs // step_every) / args['batch_size'] + 0.5)\n",
    "    scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args['epochs'] * (batches_per_epoch // args['batch_accumulate']), eta_min=1e-5)\n",
    "    scheduler2 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.001, end_factor=1.0, total_iters=args['scheduler_warmup_steps'])\n",
    "    scheduler = torch.optim.lr_scheduler.ChainedScheduler([scheduler1, scheduler2], optimizer=optimizer)\n",
    "\n",
    "if args['pretrain_use_scheduler']:\n",
    "    pretrain_batches_per_epoch = int(max_len * (num_glyphs // step_every) / args['pretrain_batch_size'] + 0.5)\n",
    "    pretrain_scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(pretrain_optimizer, T_max=args['pretrain_epochs'] * pretrain_batches_per_epoch, eta_min=1e-5)\n",
    "    pretrain_scheduler2 = torch.optim.lr_scheduler.LinearLR(pretrain_optimizer, start_factor=0.001, end_factor=1.0, total_iters=args['pretrain_scheduler_warmup_steps'])\n",
    "    pretrain_scheduler = torch.optim.lr_scheduler.ChainedScheduler([pretrain_scheduler1, pretrain_scheduler2], optimizer=pretrain_optimizer)\n",
    "\n",
    "if args['post_train_use_scheduler']:\n",
    "    posttrain_batches_per_epoch = int(max_len * (num_glyphs // step_every) / args['post_train_batch_size'] + 0.5)\n",
    "    posttrain_scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(posttrain_optimizer, T_max=args['post_train_epochs'] * posttrain_batches_per_epoch, eta_min=1e-5)\n",
    "    posttrain_scheduler2 = torch.optim.lr_scheduler.LinearLR(posttrain_optimizer, start_factor=0.001, end_factor=1.0, total_iters=args['post_train_scheduler_warmup_steps'])\n",
    "    posttrain_scheduler = torch.optim.lr_scheduler.ChainedScheduler([posttrain_scheduler1, posttrain_scheduler2], optimizer=posttrain_optimizer)\n",
    "\n",
    "if args['pretrain_vit_encoder_use_scheduler']:\n",
    "    pretrain_vit_encoder_batches_per_epoch = int(max_len * (num_glyphs // step_every) / args['pretrain_vit_encoder_batch_size'] + 0.5)\n",
    "    pretrain_vit_encoder_scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(pretrain_vit_encoder_optimizer, T_max=args['pretrain_vit_encoder_epochs'] * (pretrain_vit_encoder_batches_per_epoch // args['pretrain_vit_encoder_batch_accumulate']), eta_min=1e-5)\n",
    "    pretrain_vit_encoder_scheduler2 = torch.optim.lr_scheduler.LinearLR(pretrain_vit_encoder_optimizer, start_factor=0.001, end_factor=1.0, total_iters=args['pretrain_vit_encoder_scheduler_warmup_steps'])\n",
    "    pretrain_vit_encoder_scheduler = torch.optim.lr_scheduler.ChainedScheduler([pretrain_vit_encoder_scheduler1, pretrain_vit_encoder_scheduler2], optimizer=pretrain_vit_encoder_optimizer)\n",
    "\n",
    "dataset_name = f\"~/basic-33928allchars_centered_scaled_sorted_filtered{'_cumulative' if cumulative else ''}_padded\"\n",
    "train_start, train_end = 0, int(0.95 * max_len) * num_glyphs\n",
    "test_start, test_end = train_end, max_len * num_glyphs\n",
    "# max_len = 5\n",
    "# train_start, train_end = 0, 26*max_len\n",
    "# test_start, test_end = 0, 26*max_len\n",
    "cff_dataset = torch.load(f'./{dataset_name}.pt', mmap=True)[train_start:train_end:step_every]\n",
    "cff_dataset_test = torch.load(f'./{dataset_name}.pt', mmap=True)[test_start:test_end:step_every]\n",
    "im_dataset_name = \"~/basic-33928allchars_centered_scaled_sorted_filtered_(128, 128)\"\n",
    "im_dataset = torch.load(f'./{im_dataset_name}.pt', mmap=True)[train_start:train_end:step_every]\n",
    "im_dataset_test = torch.load(f'./{im_dataset_name}.pt', mmap=True)[test_start:test_end:step_every]\n",
    "cff_train_tensor_dataset = TensorDataset(cff_dataset, im_dataset)\n",
    "cff_train_dataloader = DataLoader(cff_train_tensor_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "cff_pretrain_dataloader = DataLoader(cff_train_tensor_dataset, batch_size=args['pretrain_batch_size'], shuffle=True)\n",
    "cff_pretrain_vit_encoder_dataloader = DataLoader(cff_train_tensor_dataset, batch_size=args['pretrain_vit_encoder_batch_size'], shuffle=True)\n",
    "cff_posttrain_dataloader = DataLoader(cff_train_tensor_dataset, batch_size=args['post_train_batch_size'], shuffle=True)\n",
    "cff_test_tensor_dataset = TensorDataset(cff_dataset_test, im_dataset_test)\n",
    "cff_test_dataloader = DataLoader(cff_test_tensor_dataset, batch_size=args['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['use_wandb']:\n",
    "    wandb.init(\n",
    "        project=\"project-typeface\",\n",
    "        config={\n",
    "            \"model_type\": \"Autoregressive CFF\",\n",
    "            **args\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(\n",
    "    reduction='sum',\n",
    "    ignore_index=tokenizer[pad_token],\n",
    "    label_smoothing=args['label_smoothing']\n",
    ")\n",
    "test_loss_fn = torch.nn.CrossEntropyLoss(\n",
    "    reduction='sum',\n",
    "    ignore_index=tokenizer[pad_token],\n",
    "    label_smoothing=0.0\n",
    ")\n",
    "def recon_loss(a, b):\n",
    "    return torch.pow((a - b), 2).sum()\n",
    "\n",
    "def mse_loss(out_scaled, target_unscaled):\n",
    "    # return torch.pow(((target_unscaled - 32) / (args['max_number'] - args['min_number']) * 2 - 1 - out_scaled) * (target_unscaled != 0), 2).sum()\n",
    "    return (((target_unscaled - 32) / (args['max_number'] - args['min_number']) * 2 - 1 - out_scaled) * (target_unscaled != 0)).abs().sum()\n",
    "\n",
    "def kl_loss_fn(mu, logvar):\n",
    "    return 0.5 * ((torch.pow(mu, 2) + logvar.exp() - logvar - 1)).sum()\n",
    "\n",
    "curve_width = 7\n",
    "curve_coeff = 1.0\n",
    "first_numeric_idx = tokenizer.special_tokens_len + len(tokenizer.possible_operators)\n",
    "zero_mask = (torch.ones((1,1,1,vocab_size)) * (torch.arange(0,vocab_size, dtype=torch.int32) >= first_numeric_idx)).to(device)\n",
    "hlf = curve_width // 2\n",
    "offset = torch.arange(-hlf, hlf+1, dtype=torch.int32)[None,None,:].to(device) # (1, 1, curve_width)\n",
    "neg_offset_exp = (-offset.abs().unsqueeze(-1) * 7.0 * curve_coeff / curve_width).exp()\n",
    "kl_loss = torch.nn.KLDivLoss(reduction='sum', log_target=False)\n",
    "log_kl_loss = torch.nn.KLDivLoss(reduction='sum', log_target=True)\n",
    "\n",
    "def numeric_mse_loss(output : torch.Tensor, targets : torch.Tensor):\n",
    "    # targets : (batch_size, seq_len)\n",
    "    with torch.no_grad():\n",
    "        is_numeric_tgt = (targets >= first_numeric_idx).unsqueeze(-1) # (batch_size, seq_len, 1)\n",
    "        is_non_padding = (targets > 0).unsqueeze(-1) # (batch_size, seq_len, 1)\n",
    "        # TODO: try removing the calculations from GPU and putting on CPU\n",
    "        token_count = targets.shape[0] * targets.shape[1]\n",
    "        arrng = torch.arange(0, token_count, dtype=torch.int32)\n",
    "        batch_indices = torch.floor_divide(arrng, targets.shape[1]).unsqueeze(0).to(device)\n",
    "        sequence_indices = torch.remainder(arrng, targets.shape[1]).unsqueeze(0).to(device)\n",
    "        token_indices = targets.flatten().unsqueeze(0)\n",
    "        single_tgt = torch.sparse_coo_tensor(\n",
    "            indices=torch.cat([batch_indices, sequence_indices, token_indices], dim=0),\n",
    "            values=torch.ones(token_count,).to(device),\n",
    "            size=(targets.shape[0], targets.shape[1], tokenizer.num_tokens)\n",
    "        ) # (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        arrng = torch.arange(0, token_count * curve_width, dtype=torch.int32)\n",
    "        batch_indices = torch.floor_divide(arrng, targets.shape[1] * curve_width).unsqueeze(0).to(device)\n",
    "        sequence_indices = torch.floor_divide(torch.remainder(arrng, targets.shape[1] * curve_width), curve_width).unsqueeze(0).to(device)\n",
    "        curve_indices = torch.remainder(arrng, curve_width).unsqueeze(0).to(device)\n",
    "        token_indices = torch.clip(targets.unsqueeze(-1) + offset, min=1, max=tokenizer.num_tokens-1).flatten().unsqueeze(0)\n",
    "        multi_tgt = torch.sparse_coo_tensor(\n",
    "            indices=torch.cat([batch_indices, sequence_indices, curve_indices, token_indices], dim=0),\n",
    "            values=torch.ones(token_count * curve_width,).to(device),\n",
    "            size=(targets.shape[0], targets.shape[1], curve_width, tokenizer.num_tokens)\n",
    "        ) # (batch_size, seq_len, curve_width, vocab_size)\n",
    "        multi_tgt = multi_tgt * neg_offset_exp * zero_mask\n",
    "        multi_tgt = (multi_tgt.sum(dim=-2).to_dense() / (1e-10 + multi_tgt.sum(dim=(-2,-1)).unsqueeze(-1).to_dense())).to_sparse() # (batch_size, seq_len, vocab_size)\n",
    "        tgt_dist = is_numeric_tgt * multi_tgt + (~is_numeric_tgt) * (is_non_padding) * single_tgt # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "    return kl_loss(output.log_softmax(dim=-1), tgt_dist.to_dense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Decoding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(epoch : int, batch_idx : int):\n",
    "    with open('./fontmakerai/.config.txt', 'r') as cf:\n",
    "        lines = cf.readlines()\n",
    "        if len(lines) != 7:\n",
    "            print(f\"Not decoding this iteration; .config.txt has wrong number of lines ({len(lines)})\")\n",
    "            return\n",
    "        else:\n",
    "            decode_instr = DecodeInstruction(\n",
    "                decode_type=DecodeType[lines[0].split(\"=\")[-1].split(\".\")[-1].strip()],\n",
    "                sampling_type=SamplingType[lines[1].split(\"=\")[-1].split(\".\")[-1].strip()],\n",
    "                max_seq_len=int(lines[2].split(\"=\")[-1].strip()),\n",
    "                k=int(lines[3].split(\"=\")[-1].strip()),\n",
    "                p=float(lines[4].split(\"=\")[-1].strip()),\n",
    "                temp=float(lines[5].split(\"=\")[-1].strip()),\n",
    "                beam_size=int(lines[6].split(\"=\")[-1].strip())\n",
    "            )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            flag = True\n",
    "            idx = np.random.randint(0, im_dataset_test.shape[0])\n",
    "            im = im_dataset_test[idx:idx+1].to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "            sequence = model.decode(im, None, decode_instr)[0].cpu().detach().numpy().flatten()\n",
    "            torch.cuda.empty_cache()\n",
    "            # sequence = cff_train_tensor_dataset[0:1][0][0].cpu().detach().numpy().flatten()#.to(device)\n",
    "            if len(sequence) == decode_instr.max_seq_len:\n",
    "                toks = [tokenizer.reverse_map(tk.item(), use_int=True) for tk in sequence] + ['endchar']\n",
    "            else:\n",
    "                toks = [tokenizer.reverse_map(tk.item(), use_int=True) for tk in sequence[:-1]]\n",
    "\n",
    "            print(\"Before:\", toks)\n",
    "            with open(f\"./fontmakerai/training_images/{epoch+1}_{batch_idx+1}.txt\", 'w') as f:\n",
    "                j_str = '\\', \\''\n",
    "                f.write(f\"Before: ['{j_str.join([str(x) for x in toks])}']\\n\\n\")\n",
    "                f.write(f\"decode_instr:\")\n",
    "                for k, v in decode_instr.__dict__.items():\n",
    "                    f.write(f\"\\n\\t{k}={v}\")\n",
    "                f.write(\"\\n\")\n",
    "            toks = [tok for tok in toks if tok != '<PAD2>' and tok != '<PAD>']\n",
    "            if cumulative:\n",
    "                toks = numbers_first(make_non_cumulative(toks, tokenizer), tokenizer, return_string=False)\n",
    "                # toks = make_non_cumulative(toks, tokenizer)\n",
    "            else:\n",
    "                toks = numbers_first(toks, tokenizer, return_string=False)\n",
    "                # toks = toks\n",
    "            print(\"After:\", toks)\n",
    "            viz = Visualizer(toks)\n",
    "            with open(f\"./fontmakerai/training_images/{epoch+1}_{batch_idx+1}.txt\", 'a', newline='\\n') as f:\n",
    "                j_str = '\\', \\''\n",
    "                f.write(f\"After: ['{j_str.join([str(x) for x in toks])}']\")\n",
    "\n",
    "            if toks[2] != \"rmoveto\" and toks[3] != \"rmoveto\":\n",
    "                raise Exception(\"first operator is not rmoveto\")\n",
    "            \n",
    "            im_pixel_size = (128, 128)\n",
    "            crop_factor = 1\n",
    "            dpi = 1\n",
    "            boundaries = (int((im_pixel_size[0] * (crop_factor * 100 / dpi - 1)) // 2), int((im_pixel_size[1] * (crop_factor * 100 / dpi - 1)) // 2))\n",
    "            im_size_inches = ((im_pixel_size[0] * crop_factor) / dpi, (im_pixel_size[1] * crop_factor) / dpi)\n",
    "            img_arr = viz.draw(\n",
    "                display=False,\n",
    "                filename=f\"./fontmakerai/training_images/{epoch+1}_{batch_idx+1}.png\",\n",
    "                return_image=True,\n",
    "                center=False,\n",
    "                im_size_inches=im_size_inches,\n",
    "                bounds=(-300, 300),\n",
    "                dpi=dpi\n",
    "            )[None,:,:,0]\n",
    "            \n",
    "            im_cpu = (im[0] * 127.5 + 127.5).to(device=device, dtype=torch.uint8).cpu().detach().numpy()\n",
    "            img_arr = wandb.Image(np.concatenate([im_cpu, img_arr], axis=2), caption=f\"epoch{epoch+1}_{batch_idx+1}.png\")\n",
    "        except Exception as e:\n",
    "            flag = False\n",
    "            print(f\"Could not generate visualization; generated output was not formatted correctly: {e}\")\n",
    "    model.train()\n",
    "    \n",
    "    if flag:\n",
    "        return (wandb.Image(im[0].to(device=device, dtype=torch.float32).cpu().detach().numpy(), caption=f\"epoch{epoch+1}_{batch_idx+1}.png\"), img_arr)\n",
    "    else:\n",
    "        return (wandb.Image(im[0].to(device=device, dtype=torch.float32).cpu().detach().numpy(), caption=f\"epoch{epoch+1}_{batch_idx+1}.png\"), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain Vision Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"pretrain_vit_encoder\"] and not args[\"use_pretrained_vit_encoder\"]:\n",
    "    print(\"\\nPretraining ViT encoder...\\n\")\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(args['pretrain_vit_encoder_epochs']):\n",
    "        total_loss = 0\n",
    "        last_loss = 0\n",
    "        pretrain_vit_encoder_optimizer.zero_grad()\n",
    "        for idx, (X, im) in enumerate(tqdm(cff_pretrain_vit_encoder_dataloader)):\n",
    "            im = im.to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "            out = model.encoder.pretrain(im)\n",
    "            loss = recon_loss(out, im) / X.shape[0]\n",
    "            total_loss += loss.item() * X.shape[0]\n",
    "            loss.backward()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if (idx+1) % args['pretrain_vit_encoder_batch_accumulate'] == 0:\n",
    "                pretrain_vit_encoder_optimizer.step()\n",
    "                pretrain_vit_encoder_optimizer.zero_grad()\n",
    "                if args['pretrain_vit_encoder_use_scheduler']:\n",
    "                    pretrain_vit_encoder_scheduler.step()\n",
    "                diff = total_loss - last_loss\n",
    "                last_loss = total_loss\n",
    "\n",
    "            if args['use_wandb']:\n",
    "                if (idx+1) % args['pretrain_vit_encoder_batch_accumulate'] == 0:\n",
    "                    wandb.log({\n",
    "                        \"pretrain_vit_encoder_loss_step\": diff / (args['pretrain_vit_encoder_batch_accumulate'] * args['pretrain_vit_encoder_batch_size']),\n",
    "                        \"pretrain_vit_encoder_lr_step\": pretrain_vit_encoder_scheduler.get_last_lr()[0] if args['pretrain_vit_encoder_use_scheduler'] else args['pretrain_vit_encoder_lr'],\n",
    "                        \"original_image\": wandb.Image(im[0].to(device=device, dtype=torch.float32).cpu().detach().numpy(), caption=f\"epoch{epoch+1}.png\"),\n",
    "                        \"reconstructed_image\": wandb.Image(out[0].clamp(-1, 1).to(device=device, dtype=torch.float32).cpu().detach().numpy(), caption=f\"epoch{epoch+1}.png\")\n",
    "                    })\n",
    "        print(f\"Epoch {epoch+1}/{args['pretrain_vit_encoder_epochs']} completed. Total Loss = {total_loss/cff_dataset.shape[0]}\")\n",
    "        pretrain_vit_encoder_optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.save(model.encoder, f'models/pretrained_vit_encoder-{args[\"embedding_dim\"]}.pt')\n",
    "elif args[\"use_pretrained_vit_encoder\"]:\n",
    "    print(\"\\nUsing pretrained ViT encoder...\\n\")\n",
    "    model.encoder = torch.load(f'models/pretrained_vit_encoder-{args[\"embedding_dim\"]}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"pretrain_embeddings\"] and not args[\"use_pretrained_embeddings\"]:\n",
    "    print(\"\\nPretraining embeddings...\\n\")\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(args['pretrain_epochs']):\n",
    "        total_loss = 0\n",
    "        for (X, im) in tqdm(cff_pretrain_dataloader):\n",
    "            pretrain_optimizer.zero_grad()\n",
    "            inputs = X.to(device, dtype=torch.int32)\n",
    "            out = model.identity_embeddings(inputs)\n",
    "            if epoch <= 0:\n",
    "                loss = numeric_mse_loss(out, inputs) / inputs.shape[0]\n",
    "            else:\n",
    "                loss = test_loss_fn(out.permute(0, 2, 1), inputs.long()) / inputs.shape[0]\n",
    "            total_loss += loss.item() * inputs.shape[0]\n",
    "            loss.backward()\n",
    "            pretrain_optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if args['pretrain_use_scheduler']:\n",
    "                pretrain_scheduler.step()\n",
    "\n",
    "            if args['use_wandb']:\n",
    "                wandb.log({\n",
    "                    \"pretrain_loss_step\": loss.item() / inputs.shape[0],\n",
    "                    \"pretrain_lr_step\": pretrain_scheduler.get_last_lr()[0] if args['pretrain_use_scheduler'] else args['pretrain_lr']\n",
    "                })\n",
    "        print(f\"Epoch {epoch+1}/{args['pretrain_epochs']} completed. Total Loss = {total_loss/cff_dataset.shape[0]}\")\n",
    "\n",
    "    torch.save(model.decoder.embedder.weight, f'models/pretrained_embeddings-{args[\"embedding_dim\"]}-numeric.pt')\n",
    "    torch.save(model.decoder.inverse_embedder.weight, f'models/pretrained_decoder_inverse_embedder-{args[\"embedding_dim\"]}-numeric.pt')\n",
    "    torch.save(model.decoder.command_encoder.weight, f'models/pretrained_command_encoder-{args[\"embedding_dim\"]}-numeric.pt')\n",
    "    torch.save(model.decoder.command_decoder.weight, f'models/pretrained_command_decoder-{args[\"embedding_dim\"]}-numeric.pt')\n",
    "    torch.save(model.decoder.norm_final.weight, f'models/pretrained_norm_final-{args[\"embedding_dim\"]}-numeric.pt')\n",
    "elif args[\"use_pretrained_embeddings\"]:\n",
    "    print(\"\\nUsing pretrained embeddings...\\n\")\n",
    "    model.decoder.embedder.weight = torch.load(f'models/pretrained_embeddings-{args[\"embedding_dim\"]}-numeric.pt', weights_only=True)\n",
    "    model.decoder.inverse_embedder.weight = torch.load(f'models/pretrained_decoder_inverse_embedder-{args[\"embedding_dim\"]}-numeric.pt', weights_only=True)\n",
    "    model.decoder.command_encoder.weight = torch.load(f'models/pretrained_command_encoder-{args[\"embedding_dim\"]}-numeric.pt', weights_only=True)\n",
    "    model.decoder.command_decoder.weight = torch.load(f'models/pretrained_command_decoder-{args[\"embedding_dim\"]}-numeric.pt', weights_only=True)\n",
    "    model.decoder.norm_final.weight = torch.load(f'models/pretrained_norm_final-{args[\"embedding_dim\"]}-numeric.pt', weights_only=True)\n",
    "\n",
    "if args['freeze_embeddings']:\n",
    "    print(\"\\nFreezing embeddings...\\n\")\n",
    "    model.decoder.embedder.weight.requires_grad = False\n",
    "    model.decoder.inverse_embedder.weight.requires_grad = False\n",
    "    model.decoder.command_encoder.weight.requires_grad = False\n",
    "    model.decoder.command_decoder.weight.requires_grad = False\n",
    "    model.decoder.norm_final.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining model...\\n\")\n",
    "\n",
    "if args['train_transformer']:\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    test_acc_list = []\n",
    "    src = torch.zeros((args['batch_size'], 0)).to(device)\n",
    "    for epoch in range(args['epochs']):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = 0\n",
    "        last_loss = 0\n",
    "        train_batches = int((max_len*(num_glyphs // step_every)*0.95) // args['batch_size']) + 1\n",
    "        # train_batches = 1000\n",
    "        for idx, (X, im) in enumerate(tqdm(cff_train_dataloader, total=train_batches)):\n",
    "            if idx >= train_batches:\n",
    "                break\n",
    "            inputs = X.to(device, dtype=torch.int32)\n",
    "            im = im.to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "            out = model(im, inputs[:,:-7]) # Use only output tokens before this truth term\n",
    "            \n",
    "            # loss = loss_fn(out.permute(0, 2, 1), inputs.long()) / X.shape[0]\n",
    "            loss = numeric_mse_loss(out, inputs) / X.shape[0]\n",
    "\n",
    "            total_loss += loss.item() * X.shape[0]\n",
    "            loss.backward()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if (idx+1) % args['batch_accumulate'] == 0 or idx == train_batches-1:\n",
    "                if args['gradient_clip']:\n",
    "                    torch.nn.utils.clip_grad_value_(model.parameters(), args['gradient_clip_val'])\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                if args['use_scheduler']:\n",
    "                    scheduler.step()\n",
    "                diff = total_loss - last_loss\n",
    "                last_loss = total_loss\n",
    "\n",
    "            if args['use_wandb']:\n",
    "                if (idx+1) % (100 * args['batch_accumulate']) == 0 or (idx == train_batches-1 and (epoch+1) % args['sample_every'] == 0):\n",
    "                    goal_image, img_arr = decode(epoch, idx)\n",
    "                    wandb.log({\n",
    "                        \"goal_image\": goal_image,\n",
    "                        \"images\": img_arr,\n",
    "                        \"train_loss_step\": diff / (args['batch_accumulate'] * args['batch_size']),\n",
    "                        \"lr_step\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                    })\n",
    "                elif (idx+1) % args['batch_accumulate'] == 0:\n",
    "                    wandb.log({\n",
    "                        \"train_loss_step\": diff / (args['batch_accumulate'] * args['batch_size']),\n",
    "                        \"lr_step\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                    })\n",
    "        # train_loss_list += [total_loss / cff_dataset.shape[0]]\n",
    "        train_loss_list += [total_loss / (min(train_batches, idx+1)*args['batch_size'])]\n",
    "        \n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        test_batches = 25\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_negatives = 0\n",
    "        with torch.no_grad():\n",
    "            for idx, (X, im) in enumerate(tqdm(cff_test_dataloader, total=test_batches)):\n",
    "                if idx >= test_batches:\n",
    "                    break\n",
    "                inputs = X.to(device, dtype=torch.int32)\n",
    "                im = im.to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "                out = model(im, inputs[:,:-7]) # Use only output tokens before this truth term\n",
    "\n",
    "                # loss = loss_fn(out.permute(0, 2, 1), inputs.long()) / X.shape[0]\n",
    "                loss = numeric_mse_loss(out, inputs) / X.shape[0]\n",
    "                \n",
    "                total_loss += loss.item() * X.shape[0]\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                guesses = out.permute(0, 2, 1).argmax(dim=1)\n",
    "                truths = inputs\n",
    "                true_positives += ((guesses == truths) * (truths != tokenizer[pad_token])).sum()\n",
    "                false_positives += ((guesses != truths) * (truths == tokenizer[pad_token])).sum()\n",
    "                true_negatives += ((guesses == truths) * (truths == tokenizer[pad_token])).sum()\n",
    "                false_negatives += ((guesses != truths) * (truths != tokenizer[pad_token])).sum()\n",
    "            \n",
    "            # test_loss_list += [total_loss / cff_dataset_test.shape[0]]\n",
    "            test_loss_list += [total_loss / (min(test_batches, idx+1)*args['batch_size'])]\n",
    "            acc, pre, rec, f1 = PerformanceMetrics.all_metrics(\n",
    "                tp=true_positives,\n",
    "                fp=false_positives,\n",
    "                tn=true_negatives,\n",
    "                fn=false_negatives\n",
    "            )\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{args['epochs']} completed. Train Loss = {train_loss_list[-1]};  Test Loss: {test_loss_list[-1]}\")\n",
    "\n",
    "            if args['use_wandb']:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": train_loss_list[-1],\n",
    "                    \"test_loss\": test_loss_list[-1],\n",
    "                    # \"test_accuracy\": acc,\n",
    "                    # \"test_precision\": pre,\n",
    "                    # \"test_recall\": rec,\n",
    "                    # \"test_f1\": f1,\n",
    "                    \"lr\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                })\n",
    "\n",
    "        # if (epoch+1) % 100 == 0 or epoch+1 == args['epochs']:\n",
    "        if max_len > 100:\n",
    "            torch.save(model, f'models/transformer-{dataset_name}-{epoch+1}.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPost-training model...\\n\")\n",
    "\n",
    "if args['post_train']:\n",
    "    # Base LLM\n",
    "    # Now, `model` is the fine-tuned policy model\n",
    "    original_model = deepcopy(model)\n",
    "    original_model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ground_truth_reward_nums(ground_truth_num_tokens):\n",
    "        '''\n",
    "        ground_truth_tokens: (batch_size, seq_len)\n",
    "        '''\n",
    "        vocab_tensor = torch.arange(0, tokenizer.num_tokens, dtype=torch.int32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        reward = (-(vocab_tensor - ground_truth_num_tokens.unsqueeze(-1)).abs()).exp() * 10\n",
    "        reward[:,:,1:35] = -10\n",
    "        reward = torch.where(ground_truth_num_tokens.unsqueeze(-1).repeat(1, 1, reward.shape[2]) == 0, torch.zeros_like(reward), reward)\n",
    "        return reward\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ground_truth_reward_ops(ground_truth_op_tokens):\n",
    "        '''\n",
    "        ground_truth_tokens: (batch_size, seq_len)\n",
    "        '''\n",
    "        vocab_tensor = torch.arange(0, tokenizer.num_tokens, dtype=torch.int32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        reward = ((vocab_tensor == ground_truth_op_tokens.unsqueeze(-1)) * 10)\n",
    "        reward[:,:,35:] = -10\n",
    "        reward = torch.where(ground_truth_op_tokens.unsqueeze(-1).repeat(1, 1, reward.shape[2]) == 0, torch.zeros_like(reward), reward)\n",
    "        return reward\n",
    "\n",
    "    # Define reward function\n",
    "    def reward_fn(ground_truth_tokens, original_policy_logits, new_policy_logits):\n",
    "        # Calculate the loss between the output and the input\n",
    "        pred_tokens = new_policy_logits.argmax(dim=-1)\n",
    "        pred_op_tokens = pred_tokens[:,::7] # (batch_size, seq_len / 7)\n",
    "        pred_num_tokens = pred_tokens.view(pred_tokens.shape[0], pred_tokens.shape[1]//7, 7)[:,:,1:].flatten(start_dim=1) # (batch_size, seq_len * 6 / 7)\n",
    "        truth_op_tokens = ground_truth_tokens[:,::7] # (batch_size, seq_len / 7)\n",
    "        truth_num_tokens = ground_truth_tokens.view(ground_truth_tokens.shape[0], ground_truth_tokens.shape[1]//7, 7)[:,:,1:].flatten(start_dim=1) # (batch_size, seq_len * 6 / 7)\n",
    "        bs = ground_truth_tokens.shape[0]\n",
    "        seq_len = ground_truth_tokens.shape[1]\n",
    "        num_logits = new_policy_logits.shape[2]\n",
    "        op_reward = ground_truth_reward_ops(truth_op_tokens)\n",
    "        num_reward = ground_truth_reward_nums(truth_num_tokens)\n",
    "        gt_rewards = torch.cat([op_reward.view((bs, seq_len//7, 1, num_logits)), num_reward.view((bs, seq_len//7, 6, num_logits))], dim=2).flatten(start_dim=1, end_dim=2) # (batch_size, seq_len, num_logits)\n",
    "        reward = (gt_rewards * new_policy_logits.softmax(dim=-1)) # (batch_size, seq_len, num_logits)\n",
    "        kl_penalty = log_kl_loss(new_policy_logits.log_softmax(dim=-1), original_policy_logits.log_softmax(dim=-1))\n",
    "        # Return the negative loss as the reward\n",
    "        return -reward.sum() + kl_penalty * args['post_train_kl_penalty']\n",
    "    \n",
    "    src = torch.zeros((args['post_train_batch_size'], 0)).to(device)\n",
    "    for epoch in range(args['post_train_epochs']):\n",
    "        model.train()\n",
    "        posttrain_optimizer.zero_grad()\n",
    "        total_loss = 0\n",
    "        last_loss = 0\n",
    "        train_batches = (max_len*(num_glyphs // step_every) // args['post_train_batch_size']) + 1\n",
    "        for idx, (X, im) in enumerate(tqdm(cff_posttrain_dataloader, total=train_batches)):\n",
    "            if idx >= train_batches:\n",
    "                break\n",
    "            inputs = X.to(device, dtype=torch.int32)\n",
    "            im = im.to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "            out_new = model.decode(im, None, decode_instr)[0].cpu().detach().numpy().flatten()\n",
    "            with torch.no_grad():\n",
    "                out_original = original_model(im, out_new[:,:-7]) # Use only output tokens before this truth term\n",
    "            \n",
    "            loss = reward_fn(inputs, out_original, out_new) / X.shape[0]\n",
    "\n",
    "            total_loss += loss.item() * X.shape[0]\n",
    "            loss.backward()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if (idx+1) % 1 == 0 or idx == train_batches-1:\n",
    "                if args['gradient_clip']:\n",
    "                    torch.nn.utils.clip_grad_value_(model.parameters(), args['gradient_clip_val'])\n",
    "                posttrain_optimizer.step()\n",
    "                posttrain_optimizer.zero_grad()\n",
    "                if args['post_train_use_scheduler']:\n",
    "                    posttrain_scheduler.step()\n",
    "                diff = total_loss - last_loss\n",
    "                last_loss = total_loss\n",
    "\n",
    "            if args['use_wandb']:\n",
    "                if (idx+1) % 100 == 0 or (idx == train_batches-1 and (epoch+1) % args['sample_every'] == 0):\n",
    "                    goal_image, img_arr = decode(epoch, idx)\n",
    "                    wandb.log({\n",
    "                        \"posttrain_goal_image\": goal_image,\n",
    "                        \"posttrain_images\": img_arr,\n",
    "                        \"posttrain_loss_step\": diff / (args['batch_accumulate'] * args['post_train_batch_size']),\n",
    "                        \"posttrain_lr_step\": args['post_train_lr'] if not args['post_train_use_scheduler'] else posttrain_scheduler.get_last_lr()[0],\n",
    "                    })\n",
    "                elif (idx+1) % 1 == 0:\n",
    "                    wandb.log({\n",
    "                        \"posttrain_loss_step\": diff / (args['batch_accumulate'] * args['post_train_batch_size']),\n",
    "                        \"posttrain_lr_step\": args['post_train_lr'] if not args['post_train_use_scheduler'] else posttrain_scheduler.get_last_lr()[0],\n",
    "                    })\n",
    "        train_loss = total_loss / (min(train_batches, idx+1)*args['batch_size'])\n",
    "        \n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        test_batches = 25\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_negatives = 0\n",
    "        with torch.no_grad():\n",
    "            for idx, (X, im) in enumerate(tqdm(cff_test_dataloader, total=test_batches)):\n",
    "                if idx >= test_batches:\n",
    "                    break\n",
    "                inputs = X.to(device, dtype=torch.int32)\n",
    "                im = im.to(dtype=args['data_type'], device=device).unsqueeze(1) / 127.5 - 1.0\n",
    "                out = model(im, inputs[:,:-7]) # Use only output tokens before this truth term\n",
    "\n",
    "                # loss = loss_fn(out.permute(0, 2, 1), inputs.long()) / X.shape[0]\n",
    "                loss = numeric_mse_loss(out, inputs) / X.shape[0]\n",
    "                \n",
    "                total_loss += loss.item() * X.shape[0]\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                guesses = out.permute(0, 2, 1).argmax(dim=1)\n",
    "                truths = inputs\n",
    "                true_positives += ((guesses == truths) * (truths != tokenizer[pad_token])).sum()\n",
    "                false_positives += ((guesses != truths) * (truths == tokenizer[pad_token])).sum()\n",
    "                true_negatives += ((guesses == truths) * (truths == tokenizer[pad_token])).sum()\n",
    "                false_negatives += ((guesses != truths) * (truths != tokenizer[pad_token])).sum()\n",
    "            \n",
    "            test_loss = total_loss / (min(test_batches, idx+1)*args['batch_size'])\n",
    "            acc, pre, rec, f1 = PerformanceMetrics.all_metrics(\n",
    "                tp=true_positives,\n",
    "                fp=false_positives,\n",
    "                tn=true_negatives,\n",
    "                fn=false_negatives\n",
    "            )\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{args['epochs']} completed. Train Loss = {train_loss_list[-1]};  Test Loss: {test_loss_list[-1]}\")\n",
    "\n",
    "            if args['use_wandb']:\n",
    "                wandb.log({\n",
    "                    \"posttrain_loss\": train_loss,\n",
    "                    \"posttrain_test_loss\": test_loss,\n",
    "                    # \"test_accuracy\": acc,\n",
    "                    # \"test_precision\": pre,\n",
    "                    # \"test_recall\": rec,\n",
    "                    # \"test_f1\": f1,\n",
    "                    \"lr\": args['lr'] if not args['use_scheduler'] else scheduler.get_last_lr()[0],\n",
    "                })\n",
    "\n",
    "        # if (epoch+1) % 100 == 0 or epoch+1 == args['epochs']:\n",
    "        if max_len > 100:\n",
    "            torch.save(model, f'models/transformer-{dataset_name}-{epoch+1}.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
